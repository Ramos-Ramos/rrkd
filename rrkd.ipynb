{
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-lightning einops torchmetrics lovely-tensors lightly wandb timm"
      ],
      "metadata": {
        "id": "dI6rQsJANGz5",
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2023-05-01T06:56:13.181166Z",
          "iopub.execute_input": "2023-05-01T06:56:13.181875Z",
          "iopub.status.idle": "2023-05-01T06:56:23.883566Z",
          "shell.execute_reply.started": "2023-05-01T06:56:13.181826Z",
          "shell.execute_reply": "2023-05-01T06:56:23.882153Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from einops.layers.torch import Rearrange\n",
        "from lightly.data import LightlyDataset\n",
        "from lightly.data import SimCLRCollateFunction\n",
        "from lightly.data.collate import imagenet_normalize\n",
        "from lightly.loss import NTXentLoss\n",
        "from lightly.models import ResNetGenerator\n",
        "from lightly.models.modules import SimCLRProjectionHead\n",
        "import lovely_tensors as lt\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchmetrics import Accuracy\n",
        "from torchvision.datasets import MNIST, CIFAR10, CIFAR100, SVHN, StanfordCars\n",
        "import torchvision.transforms as T\n",
        "import timm\n",
        "from tqdm.auto import tqdm\n",
        "import wandb\n",
        "\n",
        "from copy import deepcopy\n",
        "from functools import partial\n",
        "import inspect\n",
        "import os\n",
        "\n",
        "lt.monkey_patch()\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "VW3h4QHGJgBW",
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2023-05-01T06:56:38.731265Z",
          "iopub.execute_input": "2023-05-01T06:56:38.732387Z",
          "iopub.status.idle": "2023-05-01T06:56:51.809939Z",
          "shell.execute_reply.started": "2023-05-01T06:56:38.732338Z",
          "shell.execute_reply": "2023-05-01T06:56:51.808840Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "id": "bXMFJ3c4QTGb",
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2023-05-01T06:56:51.822239Z",
          "iopub.execute_input": "2023-05-01T06:56:51.822634Z",
          "iopub.status.idle": "2023-05-01T06:57:10.535212Z",
          "shell.execute_reply.started": "2023-05-01T06:56:51.822595Z",
          "shell.execute_reply": "2023-05-01T06:57:10.534024Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definitions"
      ],
      "metadata": {
        "id": "ymPka_QVLkpf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MNIST and CIFAR10"
      ],
      "metadata": {
        "id": "aMSWYVzKBA1L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Teachers"
      ],
      "metadata": {
        "id": "JUAj6YN-iJaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mnist_cls_teacher():\n",
        "  return nn.Sequential(\n",
        "      Rearrange('b c h w -> b (c h w)'),\n",
        "      nn.Linear(784, 1200),\n",
        "      nn.Dropout(0.5),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(1200, 1200),\n",
        "      nn.Dropout(0.5),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(1200, 10)\n",
        "  )\n",
        "\n",
        "def create_mnist_ae_teacher():\n",
        "  return nn.Sequential(\n",
        "      Rearrange('b c h w -> b (c h w)'),\n",
        "      nn.Linear(784, 128),\n",
        "      nn.Dropout(0.5),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(128, 64),\n",
        "      nn.Dropout(0.5),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(64, 128),\n",
        "      nn.Dropout(0.5),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(128, 784),\n",
        "      nn.Tanh(),\n",
        "      Rearrange('b (c h w) -> b c h w', c=1, h=28, w=28)\n",
        "  )\n",
        "\n",
        "def create_cifar10_cls_teacher():\n",
        "  return ResNetGenerator('resnet-18')\n",
        "\n",
        "def create_cifar10_simclr_teacher():\n",
        "  return nn.Sequential(\n",
        "    *list(ResNetGenerator('resnet-18').children())[:-1],\n",
        "    nn.AdaptiveAvgPool2d(1),\n",
        "    nn.Flatten()\n",
        "  )"
      ],
      "metadata": {
        "id": "HAiIIALtK3rs",
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2023-05-01T06:57:12.769627Z",
          "iopub.execute_input": "2023-05-01T06:57:12.770084Z",
          "iopub.status.idle": "2023-05-01T06:57:12.780277Z",
          "shell.execute_reply.started": "2023-05-01T06:57:12.770029Z",
          "shell.execute_reply": "2023-05-01T06:57:12.779039Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Students"
      ],
      "metadata": {
        "id": "vTbKBdmqiKnL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mnist_cls_student():\n",
        "  return nn.Sequential(\n",
        "      Rearrange('b c h w -> b (c h w)'),\n",
        "      nn.Linear(784, 32),\n",
        "      nn.Dropout(0.5),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(32, 32),\n",
        "      nn.Dropout(0.5),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(32, 10)\n",
        "  )\n",
        "\n",
        "def create_mnist_ae_student():\n",
        "  return nn.Sequential(\n",
        "      Rearrange('b c h w -> b (c h w)'),\n",
        "      nn.Linear(784, 64),\n",
        "      nn.Dropout(0.5),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(64, 32),\n",
        "      nn.Dropout(0.5),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(32, 64),\n",
        "      nn.Dropout(0.5),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(64, 784),\n",
        "      nn.Tanh(),\n",
        "      Rearrange('b (c h w) -> b c h w', c=1, h=28, w=28)\n",
        "  )\n",
        "\n",
        "def create_cifar10_cls_student():\n",
        "  return ResNetGenerator('resnet-9', width=0.5)\n",
        "\n",
        "def create_cifar10_simclr_student():\n",
        "  return nn.Sequential(\n",
        "    *list(ResNetGenerator('resnet-9', width=0.5).children())[:-1],\n",
        "    nn.AdaptiveAvgPool2d(1),\n",
        "    nn.Flatten()\n",
        "  )"
      ],
      "metadata": {
        "id": "HvDh0aS2e7zA",
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2023-05-01T06:57:13.335348Z",
          "iopub.execute_input": "2023-05-01T06:57:13.336314Z",
          "iopub.status.idle": "2023-05-01T06:57:13.346394Z",
          "shell.execute_reply.started": "2023-05-01T06:57:13.336275Z",
          "shell.execute_reply": "2023-05-01T06:57:13.345326Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Lightning"
      ],
      "metadata": {
        "id": "jooZ8LDoi1qX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LitModel(pl.LightningModule):\n",
        "  def __init__(self, model, num_classes, param_fn=lambda model : model.parameters(), lr=1e-3, momentum=0, lr_decay=None):\n",
        "    super().__init__()\n",
        "    self.model = model\n",
        "    self.val_accuracy = Accuracy(task='multiclass', num_classes=num_classes)\n",
        "    self.test_accuracy = Accuracy(task='multiclass', num_classes=num_classes)\n",
        "    self.param_fn = param_fn\n",
        "    self.lr = lr\n",
        "    self.momentum = momentum\n",
        "    self.lr_decay = lr_decay\n",
        "    self.save_hyperparameters(ignore=['model'])\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.model(x)\n",
        "\n",
        "  def step(self, batch, stage):\n",
        "    x, y = batch\n",
        "    logits = self(x)\n",
        "    loss = F.cross_entropy(logits, y)\n",
        "    self.log(f'{stage}/loss', loss, prog_bar=True)\n",
        "    return {'loss': loss, 'logits': logits, 'y': y}\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    return self.step(batch, 'train')['loss']\n",
        "\n",
        "  def test_validation_step(self, batch, stage):\n",
        "    output = self.step(batch, stage)\n",
        "    preds = torch.argmax(output['logits'], dim=1)\n",
        "    accuracy = getattr(self, f'{stage}_accuracy')\n",
        "    accuracy.update(preds, output['y'])\n",
        "    self.log(f\"{stage}/acc\", accuracy, prog_bar=True)\n",
        "    return output['loss']\n",
        "\n",
        "  def validation_step(self, batch, batch_idx):\n",
        "    return self.test_validation_step(batch, 'val')\n",
        "\n",
        "  def test_step(self, batch, batch_idx):\n",
        "    return self.test_validation_step(batch, 'test')\n",
        "\n",
        "  def predict_step(self, batch, batch_idx):\n",
        "    # can't reuse self.step() because predict doesn't support logging\n",
        "    x, y = batch\n",
        "    return {'logits': self(x), 'y': y}\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    optimizer = optim.SGD(self.param_fn(self.model), lr=self.lr, momentum=self.momentum)\n",
        "    if self.lr_decay == 'linear':\n",
        "        scheduler = optim.lr_scheduler.LinearLR(optimizer, start_factor=1, end_factor=0, total_iters=self.trainer.max_epochs)\n",
        "        scheduler_dict = {'scheduler': scheduler}\n",
        "    elif self.lr_decay == 'plateau':\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2)\n",
        "        scheduler_dict = {'scheduler': scheduler, 'monitor': 'val/loss', 'strict': True}\n",
        "    elif callable(self.lr_decay):\n",
        "        scheduler = optim.lr_scheduler.LambdaLR(optimizer, self.lr_decay)\n",
        "        scheduler_dict = {'scheduler': scheduler}\n",
        "    elif self.lr_decay is None:\n",
        "        return optimizer\n",
        "    else:\n",
        "        raise Exception(f'LR decay not supported for {self.lr_decay}')\n",
        "    return {'optimizer': optimizer, 'lr_scheduler': scheduler_dict}"
      ],
      "metadata": {
        "id": "KtDuvV-UUjSq",
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2023-05-01T06:57:13.503999Z",
          "iopub.execute_input": "2023-05-01T06:57:13.504782Z",
          "iopub.status.idle": "2023-05-01T06:57:13.520274Z",
          "shell.execute_reply.started": "2023-05-01T06:57:13.504738Z",
          "shell.execute_reply": "2023-05-01T06:57:13.518964Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LitAE(pl.LightningModule):\n",
        "  def __init__(self, model, lr=1e-3, weight_decay=0):\n",
        "    super().__init__()\n",
        "    self.model = model\n",
        "    self.lr = lr\n",
        "    self.weight_decay = weight_decay\n",
        "    self.save_hyperparameters(ignore=['model'])\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.model(x)\n",
        "\n",
        "  def step(self, batch, stage):\n",
        "    x, _ = batch\n",
        "    rec = self(x)\n",
        "    loss = F.mse_loss(rec, x)\n",
        "    self.log(f'{stage}_loss', loss, prog_bar=True)\n",
        "    return loss\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    return self.step(batch, 'train')\n",
        "\n",
        "  def validation_step(self, batch, batch_idx):\n",
        "    return self.step(batch, 'val')\n",
        "\n",
        "  def test_step(self, batch, batch_idx):\n",
        "    return self.step(batch, 'test')\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    optimizer = optim.SGD(self.parameters(), lr=self.lr, momentum=0.9, weight_decay=self.weight_decay)\n",
        "    return optimizer"
      ],
      "metadata": {
        "id": "v17c46-jEo99",
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2023-05-01T06:57:13.586413Z",
          "iopub.execute_input": "2023-05-01T06:57:13.587107Z",
          "iopub.status.idle": "2023-05-01T06:57:13.596932Z",
          "shell.execute_reply.started": "2023-05-01T06:57:13.587065Z",
          "shell.execute_reply": "2023-05-01T06:57:13.595569Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimCLR(pl.LightningModule):\n",
        "    def __init__(self, model, lr=None):\n",
        "        super().__init__()\n",
        "        self.backbone = model\n",
        "        dim = self.backbone[-3][-1].bn2.num_features\n",
        "        self.projection_head = SimCLRProjectionHead(dim, dim, 128)\n",
        "        self.criterion = NTXentLoss()\n",
        "        self.lr = lr\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x).flatten(start_dim=1)\n",
        "        z = self.projection_head(x)\n",
        "        return z\n",
        "\n",
        "    def training_step(self, batch, batch_index):\n",
        "        (x0, x1), _, _ = batch\n",
        "        z0 = self.forward(x0)\n",
        "        z1 = self.forward(x1)\n",
        "        loss = self.criterion(z0, z1)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optim = torch.optim.SGD(self.parameters(), lr=0.06 if self.lr is None else self.lr)\n",
        "        return optim"
      ],
      "metadata": {
        "id": "7gLF30b6K75j",
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2023-05-01T06:57:13.685491Z",
          "iopub.execute_input": "2023-05-01T06:57:13.686295Z",
          "iopub.status.idle": "2023-05-01T06:57:13.697101Z",
          "shell.execute_reply.started": "2023-05-01T06:57:13.686256Z",
          "shell.execute_reply": "2023-05-01T06:57:13.696051Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LitDistiller(pl.LightningModule):\n",
        "  def __init__(\n",
        "      self,\n",
        "      teacher,\n",
        "      student,\n",
        "      d_weight=1,\n",
        "      d_type='cos',\n",
        "      ce_weight=0,\n",
        "      eps=1e-8,\n",
        "      dim=None,\n",
        "      num_classes=0,\n",
        "      non_linear_head=None,\n",
        "      dropout_head=None,\n",
        "      optim='sgd',\n",
        "      lr=1e-1,\n",
        "      momentum=0.9,\n",
        "      anchors=None\n",
        "  ):\n",
        "    super().__init__()\n",
        "    self.teacher = deepcopy(teacher).eval()\n",
        "    self.student = student\n",
        "    for p in self.teacher.parameters():\n",
        "      p.requires_grad=False\n",
        "\n",
        "    self.d_weight = d_weight\n",
        "    self.ce_weight = ce_weight\n",
        "    self.eps = eps\n",
        "    self.register_buffer('anchors', anchors)\n",
        "    self.lr = lr\n",
        "    self.momentum = momentum\n",
        "    self.optim = optim\n",
        "\n",
        "    if d_type == 'cos':\n",
        "      self.d_loss_fn = lambda student_rel, teacher_rel, cos: -(cos/ 2 + 0.5 + self.eps).log().mean()\n",
        "    elif d_type == 'mse':\n",
        "      self.d_loss_fn = lambda student_rel, teacher_rel, cos: F.mse_loss(student_rel, teacher_rel)\n",
        "    elif d_type == 'sce':\n",
        "      self.d_loss_fn = lambda student_rel, teacher_rel, cos: F.cross_entropy(student_rel / 2, (teacher_rel / 2).softmax(dim=-1))\n",
        "    else:\n",
        "      raise Exception(f'`d_type` must be in `(\"cos\", \"mse\", \"sce\")`, got {d_type}')\n",
        "\n",
        "    if dim and num_classes and ce_weight and (non_linear_head is not None) and (dropout_head is not None):\n",
        "      assert non_linear_head in (True, False)\n",
        "      assert dropout_head in (True, False)\n",
        "      self.head = nn.Linear(dim, num_classes)\n",
        "      if non_linear_head:\n",
        "        self.head = nn.Sequential(\n",
        "            *([nn.Dropout(0.5)] if dropout_head else []),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(dim, num_classes)\n",
        "        )\n",
        "      self.val_accuracy = Accuracy(task='multiclass', num_classes=num_classes)\n",
        "      self.test_accuracy = Accuracy(task='multiclass', num_classes=num_classes)\n",
        "    elif dim is None and num_classes == 0 and ce_weight == 0 and non_linear_head is None:\n",
        "      self.head = None\n",
        "    else:\n",
        "      raise Exception('`dim`, `num_classes`, `ce_weight`, `non_linear_head`, and `dropout_head` must be either be all set or none set')\n",
        "\n",
        "    self.save_hyperparameters(ignore=['teacher', 'student'])\n",
        "\n",
        "  def forward(self, x):\n",
        "    # print(self.teacher.training)\n",
        "    return {'teacher': self.teacher(x), 'student': self.student(x)}\n",
        "\n",
        "  def step(self, batch, stage):\n",
        "    ret_dict = {}\n",
        "\n",
        "    # get teacher and student emebddings\n",
        "    x, y = batch\n",
        "    features = self(x)\n",
        "    teacher_abs = features['teacher']\n",
        "    student_abs = features['student']\n",
        "\n",
        "    # auxiliary loss can be cross entropy with labels\n",
        "    # only works when distilling class probabilities\n",
        "    ce_loss = 0\n",
        "    if self.ce_weight:\n",
        "      student_logits = self.head(student_abs)\n",
        "      ce_loss = F.cross_entropy(student_logits, y)\n",
        "      self.log(f'{stage}/ce_loss', ce_loss, prog_bar=True, sync_dist=True)\n",
        "      ret_dict.update({'student_logits': student_logits})\n",
        "\n",
        "    # anchors are either the embeddings themselves or predefined anchors\n",
        "    teacher_anchors, student_anchors = teacher_abs, student_abs\n",
        "    if self.anchors is not None:\n",
        "      teacher_anchors = self.teacher(self.anchors)\n",
        "      student_anchors = self.student(self.anchors)\n",
        "\n",
        "    # normalize anchors\n",
        "    teacher_anchors = teacher_anchors / teacher_anchors.norm(dim=-1, keepdim=True)\n",
        "    student_anchors = student_anchors / student_anchors.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    # normalize absolute representations\n",
        "    teacher_abs = teacher_abs / teacher_abs.norm(dim=-1, keepdim=True)\n",
        "    student_abs = student_abs / student_abs.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    # compute relative representations\n",
        "    teacher_rel = teacher_abs.mm(teacher_anchors.T)\n",
        "    student_rel = student_abs.mm(student_anchors.T)\n",
        "\n",
        "    # actual distillation loss\n",
        "    if self.d_weight:\n",
        "      cos = F.cosine_similarity(student_rel, teacher_rel)\n",
        "      d_loss = self.d_loss_fn(student_rel, teacher_rel, cos)\n",
        "#       d_loss = -(cos/ 2 + 0.5 + self.eps).log().mean()\n",
        "#       d_loss = F.mse_loss(student_rel, teacher_rel)\n",
        "#       d_loss = F.cross_entropy(student_rel / 2, (teacher_rel / 2).softmax(dim=-1))\n",
        "      self.log(f'{stage}/cos', cos.mean(), prog_bar=True, sync_dist=True)\n",
        "      self.log(f'{stage}/d_loss', d_loss, prog_bar=True, sync_dist=True)\n",
        "    else:\n",
        "      d_loss = 0\n",
        "\n",
        "    # loss = self.d_weight * d_loss + self.ce_weight * ce_loss + self.sce_weight * sce_loss\n",
        "    loss = self.d_weight * d_loss + self.ce_weight * ce_loss\n",
        "    self.log(f'{stage}/loss', loss, sync_dist=True)\n",
        "\n",
        "    ret_dict.update({'loss': loss, 'features': features, 'y': y})\n",
        "    return ret_dict\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    # assert not self.teacher.training\n",
        "    # for p in self.teacher.parameters():\n",
        "    #   assert not p.requires_grad\n",
        "    return self.step(batch, 'train')['loss']\n",
        "\n",
        "  def test_validation_step(self, batch, stage):\n",
        "    output = self.step(batch, stage)\n",
        "    if self.head is not None:\n",
        "      preds = torch.argmax(output['student_logits'], dim=1)\n",
        "      accuracy = getattr(self, f'{stage}_accuracy')\n",
        "      accuracy.update(preds, output['y'])\n",
        "      self.log(\"val/acc\", accuracy, prog_bar=True, sync_dist=True)\n",
        "    return output['loss']\n",
        "\n",
        "  def validation_step(self, batch, batch_idx):\n",
        "    return self.test_validation_step(batch, 'val')\n",
        "\n",
        "  def test_step(self, batch, batch_idx):\n",
        "    return self.test_validation_step(batch, 'test')\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    if self.optim == 'sgd':\n",
        "        optim_cls = optim.SGD\n",
        "    elif self.optim == 'adam':\n",
        "        optim_cls = optim.Adam\n",
        "    optimizer = optim_cls(self.student.parameters(), lr=self.lr, momentum=self.momentum)\n",
        "    return optimizer\n",
        "\n",
        "  def on_train_start(self):\n",
        "    self.teacher.eval()\n",
        "    print(f'd_weight: {self.d_weight}\\tce_weight: {self.ce_weight}')\n",
        "    print(f'anchors: {self.anchors}')\n",
        "    print(f'head: {self.head}')\n",
        "    print(f'optim: {self.optim}')"
      ],
      "metadata": {
        "id": "bYHpepHMmUkB",
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2023-05-01T06:57:13.756714Z",
          "iopub.execute_input": "2023-05-01T06:57:13.757057Z",
          "iopub.status.idle": "2023-05-01T06:57:13.785274Z",
          "shell.execute_reply.started": "2023-05-01T06:57:13.757027Z",
          "shell.execute_reply": "2023-05-01T06:57:13.783980Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LitSPDistiller(pl.LightningModule):\n",
        "  def __init__(\n",
        "      self,\n",
        "      teacher,\n",
        "      student,\n",
        "      lr=1e-3,\n",
        "      momentum=0.9,\n",
        "      d_weight=1,\n",
        "      ce_weight=0,\n",
        "      num_classes=0,\n",
        "      dim=None\n",
        "  ):\n",
        "    super().__init__()\n",
        "\n",
        "    # set up teacher and student\n",
        "    self.teacher = deepcopy(teacher).eval()\n",
        "    self.student = student\n",
        "    for p in self.teacher.parameters():\n",
        "      p.requires_grad=False\n",
        "\n",
        "    # optimization\n",
        "    self.lr = lr\n",
        "    self.momentum = momentum\n",
        "\n",
        "    # loss components\n",
        "    self.d_weight = d_weight\n",
        "    self.ce_weight = ce_weight\n",
        "\n",
        "    # accuracy if distilling with class labels\n",
        "    if ce_weight and dim is not None and num_classes:\n",
        "      self.head = nn.Linear(dim, num_classes)\n",
        "      self.val_accuracy = Accuracy(task='multiclass', num_classes=num_classes)\n",
        "      self.test_accuracy = Accuracy(task='multiclass', num_classes=num_classes)\n",
        "    elif ce_weight == 0 and dim is None and num_classes == 0:\n",
        "      self.head = None\n",
        "    else:\n",
        "      raise Exception('`dim`, `num_classes`, and `ce_weight` must be either be all set or none set')\n",
        "    self.save_hyperparameters(ignore=['teacher', 'student'])\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    # print(self.teacher.training)\n",
        "    return {'teacher': self.teacher(x), 'student': self.student(x)}\n",
        "\n",
        "  def step(self, batch, stage):\n",
        "    ret_dict = {}\n",
        "\n",
        "    # get teacher and student emebddings\n",
        "    x, y = batch\n",
        "    logits = self(x)\n",
        "    teacher_abs = logits['teacher']\n",
        "    student_abs = logits['student']\n",
        "\n",
        "    ce_loss = 0\n",
        "    if self.ce_weight > 0:\n",
        "      student_logits = self.head(student_abs)\n",
        "      ce_loss = F.cross_entropy(student_logits, y)\n",
        "      self.log(f'{stage}/ce_loss', ce_loss, prog_bar=True)\n",
        "      ret_dict.update({'student_logits': student_logits})\n",
        "\n",
        "    # compute relative representations\n",
        "    teacher_rel = teacher_abs.mm(teacher_abs.T)\n",
        "    student_rel = student_abs.mm(student_abs.T)\n",
        "\n",
        "    # l2 normalize relative representations\n",
        "    teacher_rel = teacher_rel / teacher_rel.norm(dim=1, keepdim=True)\n",
        "    student_rel = student_rel / student_rel.norm(dim=1, keepdim=True)\n",
        "\n",
        "    d_loss = F.mse_loss(teacher_rel, student_rel)\n",
        "    self.log(f'{stage}/d_loss', d_loss, prog_bar=True)\n",
        "\n",
        "    loss = self.d_weight * d_loss + self.ce_weight * ce_loss\n",
        "    self.log(f'{stage}/loss', loss, prog_bar=True)\n",
        "\n",
        "    ret_dict.update({'loss': loss, 'y': y})\n",
        "    return ret_dict\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    # assert not self.teacher.training\n",
        "    # for p in self.teacher.parameters():\n",
        "    #   assert not p.requires_grad\n",
        "    return self.step(batch, 'train')['loss']\n",
        "\n",
        "  def test_validation_step(self, batch, stage):\n",
        "    output = self.step(batch, stage)\n",
        "    if self.head is not None:\n",
        "      preds = torch.argmax(output['student_logits'], dim=1)\n",
        "      accuracy = getattr(self, f'{stage}_accuracy')\n",
        "      accuracy.update(preds, output['y'])\n",
        "      self.log(f\"{stage}/acc\", accuracy, prog_bar=True)\n",
        "    return output['loss']\n",
        "\n",
        "  def validation_step(self, batch, batch_idx):\n",
        "    return self.test_validation_step(batch, 'val')\n",
        "\n",
        "  def test_step(self, batch, batch_idx):\n",
        "    return self.test_validation_step(batch, 'test')\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    optimizer = optim.SGD(self.student.parameters(), lr=self.lr, momentum=self.momentum, nesterov=True)\n",
        "    return optimizer\n",
        "\n",
        "  def on_train_start(self):\n",
        "    self.teacher.eval()\n",
        "    print(f'd_weight: {self.d_weight}\\tce_weight: {self.ce_weight}')\n",
        "    print(f'head: {self.head}')"
      ],
      "metadata": {
        "id": "r3B8UgPXRyGL",
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2023-05-01T06:57:13.835145Z",
          "iopub.execute_input": "2023-05-01T06:57:13.835426Z",
          "iopub.status.idle": "2023-05-01T06:57:13.855698Z",
          "shell.execute_reply.started": "2023-05-01T06:57:13.835398Z",
          "shell.execute_reply": "2023-05-01T06:57:13.854509Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LitLPDistiller(pl.LightningModule):\n",
        "  def __init__(\n",
        "      self,\n",
        "      teacher,\n",
        "      student,\n",
        "      lr,\n",
        "      momentum=0.9,\n",
        "      weight_decay=0,\n",
        "      d_weight=1.5,\n",
        "      k=5,\n",
        "      normalizing_constant=1,\n",
        "      ce_weight=1,\n",
        "      num_classes=0,\n",
        "      dim=None,\n",
        "      sce_weight=2,\n",
        "      temp=0.5,\n",
        "      teacher_head=None\n",
        "  ):\n",
        "    super().__init__()\n",
        "    self.teacher = deepcopy(teacher).eval()\n",
        "    self.student = student\n",
        "    for p in self.teacher.parameters():\n",
        "      p.requires_grad=False\n",
        "\n",
        "    # optimization\n",
        "    self.lr = lr\n",
        "    self.weight_decay = weight_decay\n",
        "    self.momentum = momentum\n",
        "\n",
        "    # loss components\n",
        "    self.d_weight = d_weight\n",
        "    self.ce_weight = ce_weight\n",
        "    self.sce_weight = sce_weight\n",
        "\n",
        "    # lp distillation\n",
        "    self.k = k\n",
        "    self.normalizing_constant = normalizing_constant\n",
        "\n",
        "    # accuracy if distilling with class labels\n",
        "    if (ce_weight or sce_weight) and dim is not None and num_classes:\n",
        "      self.student_head = nn.Linear(dim, num_classes)\n",
        "\n",
        "      if sce_weight and teacher_head:\n",
        "        self.teacher_head = teacher_head\n",
        "        for p in self.teacher_head.parameters():\n",
        "          p.requires_grad=False\n",
        "      elif not (sce_weight == 0 and teacher_head is None):\n",
        "        raise Exception('`sce_weight` and `teacher_head` must be all set or none set')\n",
        "\n",
        "      self.val_accuracy = Accuracy(task='multiclass', num_classes=num_classes)\n",
        "      self.test_accuracy = Accuracy(task='multiclass', num_classes=num_classes)\n",
        "    elif ce_weight == 0 and sce_weight == 0 and dim is None and num_classes == 0 and teacher_head is None:\n",
        "      self.student_head = None\n",
        "      self.teacher_head = None\n",
        "    else:\n",
        "      raise Exception('`dim` and `num_classes` and either `ce_weight` or `sce_weight` must be either be all set or none set')\n",
        "\n",
        "    # sce\n",
        "    self.temp = temp\n",
        "\n",
        "    self.save_hyperparameters(ignore=['teacher', 'student', 'teacher_head'])\n",
        "\n",
        "  def forward(self, x):\n",
        "    # print(self.teacher.training)\n",
        "    return {'teacher': self.teacher(x), 'student': self.student(x)}\n",
        "\n",
        "  def step(self, batch, stage):\n",
        "    ret_dict = {}\n",
        "\n",
        "    # get teacher and student emebddings\n",
        "    x, y = batch\n",
        "    logits = self(x)\n",
        "    teacher_abs = logits['teacher']\n",
        "    student_abs = logits['student']\n",
        "\n",
        "    if self.student_head:\n",
        "      student_logits = self.student_head(student_abs)\n",
        "      ret_dict.update({'student_logits': student_logits})\n",
        "\n",
        "    ce_loss = 0\n",
        "    if self.ce_weight > 0:\n",
        "      ce_loss = F.cross_entropy(student_logits, y)\n",
        "      self.log(f'{stage}/ce_loss', ce_loss, prog_bar=True)\n",
        "\n",
        "    sce_loss = 0\n",
        "    if self.sce_weight > 0:\n",
        "      teacher_logits = self.teacher_head(teacher_abs)\n",
        "      sce_loss = F.cross_entropy(student_logits, teacher_logits.softmax(dim=1))\n",
        "      self.log(f'{stage}/sce_loss', sce_loss, prog_bar=True)\n",
        "\n",
        "    d_loss = 0\n",
        "    if self.d_weight:\n",
        "        # compute teacher_map\n",
        "        teacher_rel = torch.cdist(teacher_abs, teacher_abs).pow(2)\n",
        "        knn, knn_ids = teacher_rel.sort()\n",
        "        knn, knn_ids = knn[:, 1:1+self.k], knn_ids[:, 1:1+self.k]\n",
        "        knn = (-knn / self.normalizing_constant ** 2).exp()\n",
        "        teacher_rel = torch.zeros_like(teacher_rel).scatter_(1, knn_ids, knn)\n",
        "\n",
        "        # compute student map\n",
        "        student_rel = torch.cdist(student_abs, student_abs).pow(2)\n",
        "\n",
        "        d_loss = (teacher_rel * student_rel).sum() / self.k\n",
        "        self.log(f'{stage}/d_loss', d_loss)\n",
        "\n",
        "    loss = self.d_weight * d_loss + self.ce_weight * ce_loss + self.sce_weight * sce_loss\n",
        "    self.log(f'{stage}/loss', loss, prog_bar=True)\n",
        "\n",
        "    ret_dict.update({'loss': loss, 'y': y})\n",
        "    return ret_dict\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    # assert not self.teacher.training\n",
        "    # for p in self.teacher.parameters():\n",
        "    #   assert not p.requires_grad\n",
        "    return self.step(batch, 'train')['loss']\n",
        "\n",
        "  def test_validation_step(self, batch, stage):\n",
        "    output = self.step(batch, stage)\n",
        "    if self.student_head is not None:\n",
        "      preds = torch.argmax(output['student_logits'], dim=1)\n",
        "      accuracy = getattr(self, f'{stage}_accuracy')\n",
        "      accuracy.update(preds, output['y'])\n",
        "      self.log(f\"{stage}/acc\", accuracy, prog_bar=True)\n",
        "    return output['loss']\n",
        "\n",
        "  def validation_step(self, batch, batch_idx):\n",
        "    return self.test_validation_step(batch, 'val')\n",
        "\n",
        "  def test_step(self, batch, batch_idx):\n",
        "    self.test_validation_step(batch, 'test')\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    optimizer = optim.SGD(self.student.parameters(), lr=self.lr, momentum=self.momentum, weight_decay=self.weight_decay)\n",
        "    return optimizer\n",
        "\n",
        "  def on_train_start(self):\n",
        "    self.teacher.eval()\n",
        "    if self.teacher_head is not None:\n",
        "      self.teacher_head.eval()\n",
        "    print(f'd_weight: {self.d_weight}\\tce_weight: {self.ce_weight}\\tsce_weight: {self.sce_weight}')\n",
        "    print(f'student_head: {self.student_head}')\n",
        "    print(f'teacher_head: {self.teacher_head}')"
      ],
      "metadata": {
        "id": "ffI7quK4xaXD",
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2023-05-01T06:57:13.921643Z",
          "iopub.execute_input": "2023-05-01T06:57:13.922002Z",
          "iopub.status.idle": "2023-05-01T06:57:13.948302Z",
          "shell.execute_reply.started": "2023-05-01T06:57:13.921933Z",
          "shell.execute_reply": "2023-05-01T06:57:13.947029Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LitStandardDistiller(pl.LightningModule):\n",
        "  def __init__(\n",
        "      self,\n",
        "      teacher,\n",
        "      student,\n",
        "      lr,\n",
        "      temp=1,\n",
        "      d_weight=1,\n",
        "      ce_weight=1,\n",
        "      num_classes=0,\n",
        "      teacher_head=None\n",
        "  ):\n",
        "    super().__init__()\n",
        "    self.teacher = deepcopy(teacher).eval()\n",
        "    self.student = student\n",
        "    for p in self.teacher.parameters():\n",
        "      p.requires_grad=False\n",
        "\n",
        "    # optimization\n",
        "    self.lr = lr\n",
        "\n",
        "    # loss components\n",
        "    self.d_weight = d_weight\n",
        "    self.ce_weight = ce_weight\n",
        "\n",
        "    # distillation\n",
        "    self.temp = temp\n",
        "\n",
        "    # accuracy\n",
        "    self.val_accuracy = Accuracy(task='multiclass', num_classes=num_classes)\n",
        "    self.test_accuracy = Accuracy(task='multiclass', num_classes=num_classes)\n",
        "\n",
        "    self.save_hyperparameters(ignore=['teacher', 'student', 'teacher_head'])\n",
        "\n",
        "  def forward(self, x):\n",
        "    # print(self.teacher.training)\n",
        "    return {'teacher': self.teacher(x), 'student': self.student(x)}\n",
        "\n",
        "  def step(self, batch, stage):\n",
        "    ret_dict = {}\n",
        "\n",
        "    # get teacher and student emebddings\n",
        "    x, y = batch\n",
        "    logits = self(x)\n",
        "    teacher_logits = logits['teacher']\n",
        "    student_logits = logits['student']\n",
        "    ret_dict.update({'student_logits': student_logits})\n",
        "\n",
        "    ce_loss = 0\n",
        "    if self.ce_weight > 0:\n",
        "      ce_loss = F.cross_entropy(student_logits, y)\n",
        "      self.log(f'{stage}/ce_loss', ce_loss, prog_bar=True)\n",
        "\n",
        "    d_loss = 0\n",
        "    if self.sce_weight > 0:\n",
        "      d_loss = F.cross_entropy(student_logits / self.temp, (teacher_logits / self.temp).softmax(dim=1)) /(self.temp ** 2)\n",
        "      self.log(f'{stage}/d_loss', sce_loss, prog_bar=True)\n",
        "\n",
        "    loss = self.d_weight * d_loss + self.ce_weight * ce_loss\n",
        "    self.log(f'{stage}/loss', loss, prog_bar=True)\n",
        "\n",
        "    ret_dict.update({'loss': loss, 'y': y})\n",
        "    return ret_dict\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    # assert not self.teacher.training\n",
        "    # for p in self.teacher.parameters():\n",
        "    #   assert not p.requires_grad\n",
        "    return self.step(batch, 'train')['loss']\n",
        "\n",
        "  def test_validation_step(self, batch, stage):\n",
        "    output = self.step(batch, stage)\n",
        "    if self.student_head is not None:\n",
        "      preds = torch.argmax(output['student_logits'], dim=1)\n",
        "      accuracy = getattr(self, f'{stage}_accuracy')\n",
        "      accuracy.update(preds, output['y'])\n",
        "      self.log(\"val/acc\", accuracy, prog_bar=True)\n",
        "    return output['loss']\n",
        "\n",
        "  def validation_step(self, batch, batch_idx):\n",
        "    return self.test_validation_step(batch, 'val')\n",
        "\n",
        "  def test_step(self, batch, batch_idx):\n",
        "    self.test_validation_step(batch, 'test')\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    optimizer = optim.SGD(self.student.parameters(), lr=self.lr, momentum=0.9)\n",
        "    return optimizer\n",
        "\n",
        "  def on_train_start(self):\n",
        "    self.teacher.eval()\n",
        "    print(f'd_weight: {self.d_weight}\\tce_weight: {self.ce_weight}')\n",
        "    print(f'student_head: {self.student_head}')\n",
        "    print(f'teacher_head: {self.teacher_head}')"
      ],
      "metadata": {
        "id": "phu8T5CAybcc",
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2023-05-01T06:57:14.011326Z",
          "iopub.execute_input": "2023-05-01T06:57:14.011605Z",
          "iopub.status.idle": "2023-05-01T06:57:14.030926Z",
          "shell.execute_reply.started": "2023-05-01T06:57:14.011578Z",
          "shell.execute_reply": "2023-05-01T06:57:14.029530Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data"
      ],
      "metadata": {
        "id": "X0qkSxnwjNJI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GenericDataModule(pl.LightningDataModule):\n",
        "\n",
        "  def __init__(self, ds_class, data_dir='./', batch_size=512, timm_name=None):\n",
        "    super().__init__()\n",
        "    self.ds_class = ds_class\n",
        "    self.data_dir = data_dir\n",
        "    self.batch_size = batch_size\n",
        "    if timm_name is not None:\n",
        "      model = timm.create_model(timm_name, num_classes=0)\n",
        "      data_config = timm.data.config.resolve_data_config(model.pretrained_cfg)\n",
        "      self.train_transform = timm.data.create_transform(**data_config, is_training=True)\n",
        "      self.test_transform = timm.data.create_transform(**data_config, is_training=False)\n",
        "    else:\n",
        "      self.train_transform = T.Compose(\n",
        "          [\n",
        "              T.ToTensor(),\n",
        "              T.Normalize(0.5, 0.5),\n",
        "              *([T.RandomResizedCrop(224)] if ds_class in (StanfordCars,) else []),\n",
        "              *([T.RandomHorizontalFlip()] if ds_class in (CIFAR10, CIFAR100,) else [])\n",
        "          ]\n",
        "      )\n",
        "      self.test_transform = T.Compose(\n",
        "          [\n",
        "              T.ToTensor(),\n",
        "              T.Normalize(0.5, 0.5),\n",
        "              *([T.CenterCrop(224)] if ds_class in (StanfordCars,) else [])\n",
        "          ]\n",
        "      )\n",
        "    parameters = inspect.signature(self.ds_class).parameters\n",
        "    # kaggle being kaggle doesn't make the parameters show up right\n",
        "    if 'train' in parameters or ds_class in [MNIST, CIFAR10, CIFAR100]:\n",
        "      self.train_split_kwargs = {'train': True}\n",
        "      self.test_split_kwargs = {'train': False}\n",
        "    elif 'split' in parameters or ds_class in [SVHN, StanfordCars]:\n",
        "      self.train_split_kwargs = {'split': 'train'}\n",
        "      self.test_split_kwargs = {'split': 'test'}\n",
        "    else:\n",
        "      raise Exception(f'Account for dataset {ds_class}')\n",
        "\n",
        "\n",
        "  def prepare_data(self):\n",
        "    self.ds_class(self.data_dir, download=True, **self.train_split_kwargs)\n",
        "    self.ds_class(self.data_dir, download=True, **self.test_split_kwargs)\n",
        "\n",
        "  def setup(self, stage=None):\n",
        "    if stage == \"fit\" or stage is None:\n",
        "      ds_full = self.ds_class(self.data_dir, transform=self.train_transform, **self.train_split_kwargs)\n",
        "      self.ds_train, _ = random_split(ds_full, [0.9, 0.1], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "    if stage in (\"fit\", \"validate\") or stage is None:\n",
        "      ds_full = self.ds_class(self.data_dir, transform=self.test_transform, **self.test_split_kwargs)\n",
        "      _, self.ds_val = random_split(ds_full, [0.9, 0.1], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "    if stage == \"test\" or stage is None:\n",
        "      self.ds_test = self.ds_class(self.data_dir, transform=self.test_transform, **self.test_split_kwargs)\n",
        "\n",
        "  def train_dataloader(self):\n",
        "      return DataLoader(\n",
        "          self.ds_train,\n",
        "          batch_size=self.batch_size,\n",
        "          shuffle=True,\n",
        "          generator=torch.Generator().manual_seed(12345678),\n",
        "      )\n",
        "\n",
        "  def val_dataloader(self):\n",
        "      return DataLoader(self.ds_val, batch_size=self.batch_size, shuffle=False)\n",
        "\n",
        "  def test_dataloader(self):\n",
        "      return DataLoader(self.ds_test, batch_size=self.batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "lAHOG17GHuBY",
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2023-05-01T06:57:14.171144Z",
          "iopub.execute_input": "2023-05-01T06:57:14.171866Z",
          "iopub.status.idle": "2023-05-01T06:57:14.190329Z",
          "shell.execute_reply.started": "2023-05-01T06:57:14.171827Z",
          "shell.execute_reply": "2023-05-01T06:57:14.188921Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimCLRCIFAR10DataModule(pl.LightningDataModule):\n",
        "  def __init__(self, data_dir='./', batch_size=512):\n",
        "    super().__init__()\n",
        "    self.data_dir = data_dir\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "  def prepare_data(self):\n",
        "    CIFAR10(self.data_dir, train=True, download=True)\n",
        "\n",
        "  def setup(self, stage=None):\n",
        "    self.ssl_ds_train = LightlyDataset.from_torch_dataset(\n",
        "        CIFAR10(self.data_dir, download=True, train=True)\n",
        "    )\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    collate_fn = SimCLRCollateFunction(\n",
        "      input_size=32,\n",
        "      gaussian_blur=0.,\n",
        "    )\n",
        "    return DataLoader(\n",
        "      self.ssl_ds_train,\n",
        "      batch_size=self.batch_size,\n",
        "      collate_fn=collate_fn,\n",
        "      shuffle=True,\n",
        "      drop_last=True,\n",
        "      num_workers=2,\n",
        "      generator=torch.Generator().manual_seed(12345678)\n",
        "    )"
      ],
      "metadata": {
        "id": "QwBaoUssMQRh",
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2023-05-01T06:57:14.521547Z",
          "iopub.execute_input": "2023-05-01T06:57:14.522466Z",
          "iopub.status.idle": "2023-05-01T06:57:14.531326Z",
          "shell.execute_reply.started": "2023-05-01T06:57:14.522416Z",
          "shell.execute_reply": "2023-05-01T06:57:14.530030Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_64 = {'ds_class': MNIST, 'batch_size': 64}\n",
        "mnist_1024 = {'ds_class': MNIST, 'batch_size': 1024}\n",
        "cifar10_64 = {'ds_class': CIFAR10, 'batch_size': 64}\n",
        "cifar10_1024 = {'ds_class': CIFAR10, 'batch_size': 1024}\n",
        "cifar10_256 = {'ds_class': CIFAR10, 'batch_size': 256}"
      ],
      "metadata": {
        "id": "wJlhtwTNnEHy",
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2023-05-01T06:57:14.714026Z",
          "iopub.execute_input": "2023-05-01T06:57:14.714751Z",
          "iopub.status.idle": "2023-05-01T06:57:14.721541Z",
          "shell.execute_reply.started": "2023-05-01T06:57:14.714711Z",
          "shell.execute_reply": "2023-05-01T06:57:14.720211Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trainer"
      ],
      "metadata": {
        "id": "vtr5utVUvY0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "create_trainer = partial(\n",
        "    pl.Trainer,\n",
        "    accelerator='auto',\n",
        "    max_epochs=10,\n",
        "    deterministic=True\n",
        ")"
      ],
      "metadata": {
        "id": "uxLH0RZ003U-",
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2023-05-01T06:57:15.230898Z",
          "iopub.execute_input": "2023-05-01T06:57:15.231765Z",
          "iopub.status.idle": "2023-05-01T06:57:15.237267Z",
          "shell.execute_reply.started": "2023-05-01T06:57:15.231705Z",
          "shell.execute_reply": "2023-05-01T06:57:15.235932Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def quick_train(\n",
        "    model_init,\n",
        "    lit_model_cls,\n",
        "    dm_init,\n",
        "    lit_model_kwargs={},\n",
        "    dm_kwargs={},\n",
        "    trainer_kwargs={},\n",
        "    seed=12345678,\n",
        "    project=None,\n",
        "    name=None,\n",
        "    id=None,\n",
        "    log_model=False\n",
        "):\n",
        "  fit_kwargs = {}\n",
        "  if project is not None and name is not None:\n",
        "    resume_kwargs = {'id': id, 'resume': 'allow'} if id is not None else {}\n",
        "    wandb.finish()\n",
        "    if id is not None and log_model:\n",
        "        api = wandb.Api()\n",
        "        artifact = api.artifact(f'patrickramosobf/{project}/model-{id}:latest')\n",
        "        fit_kwargs['ckpt_path'] = artifact.download() + '/model.ckpt'\n",
        "    wandb_logger = WandbLogger(\n",
        "        project=project,\n",
        "        name=name,\n",
        "        log_model=log_model,\n",
        "        # entity=,\n",
        "        **resume_kwargs\n",
        "    )\n",
        "    trainer_kwargs['logger'] = wandb_logger\n",
        "  trainer_kwargs['callbacks'] = [LearningRateMonitor()]\n",
        "  pl.seed_everything(seed)\n",
        "  dm = dm_init(**dm_kwargs)\n",
        "  model = model_init()\n",
        "  lit_model = lit_model_cls(model, **lit_model_kwargs)\n",
        "  trainer = create_trainer(**trainer_kwargs)\n",
        "  trainer.fit(lit_model, dm, **fit_kwargs)\n",
        "  return lit_model\n",
        "\n",
        "def quick_distill(\n",
        "    student_init,\n",
        "    dm_init,\n",
        "    dm_kwargs={},\n",
        "    distill_cls=LitDistiller,\n",
        "    distill_kwargs={},\n",
        "    trainer_kwargs={},\n",
        "    student_preprocess=lambda model: model,\n",
        "    seed=12345678,\n",
        "    project=None,\n",
        "    name=None,\n",
        "    id=None,\n",
        "    log_model=False\n",
        "):\n",
        "  fit_kwargs = {}\n",
        "  if project is not None and name is not None:\n",
        "    resume_kwargs = {'id': id, 'resume': 'allow'} if id is not None else {}\n",
        "    wandb.finish()\n",
        "    if id is not None and log_model:\n",
        "        api = wandb.Api()\n",
        "        artifact = api.artifact(f'patrickramosobf/{project}/model-{id}:latest')\n",
        "        fit_kwargs['ckpt_path'] = artifact.download() + '/model.ckpt'\n",
        "    wandb_logger = WandbLogger(\n",
        "        project=project,\n",
        "        name=name,\n",
        "        log_model=log_model,\n",
        "        # entity=,\n",
        "        **resume_kwargs\n",
        "    )\n",
        "    trainer_kwargs['logger'] = wandb_logger\n",
        "  pl.seed_everything(seed)\n",
        "  dm = dm_init(**dm_kwargs)\n",
        "  distilled = student_preprocess(student_init())\n",
        "  lit_distiller = distill_cls(\n",
        "      student=distilled, **distill_kwargs\n",
        "  )\n",
        "  trainer = create_trainer(**trainer_kwargs)\n",
        "  trainer.fit(lit_distiller, dm, **fit_kwargs)\n",
        "  return lit_distiller\n",
        "\n",
        "def quick_fc_probe(\n",
        "    model,\n",
        "    dm_init,\n",
        "    lit_model_kwargs={},\n",
        "    dm_kwargs={},\n",
        "    model_preprocess=lambda model: model,\n",
        "    seed=12345678,\n",
        "):\n",
        "  '''linear probe eval with trainable fully-connected layer'''\n",
        "  pl.seed_everything(seed)\n",
        "  dm = dm_init(**dm_kwargs)\n",
        "  linear = model_preprocess(deepcopy(model))\n",
        "  linear.extend([nn.ReLU(), nn.Linear(linear[-1].out_features, 10)])\n",
        "  linear.eval()\n",
        "  for p in linear[:-2].parameters():\n",
        "    p.requires_grad = False\n",
        "  lit_linear = LitModel(\n",
        "      linear, 10, param_fn=lambda model: model[-1].parameters(), **lit_model_kwargs\n",
        "  )\n",
        "  trainer = create_trainer()\n",
        "  trainer.fit(lit_linear, dm)\n",
        "  return lit_linear\n",
        "\n",
        "def quick_sk_probe(\n",
        "    model,\n",
        "    dm_init,\n",
        "    dm_kwargs={},\n",
        "    sk_kwargs={},\n",
        "    model_preprocess=lambda model: model,\n",
        "    seed=12345678,\n",
        "    project=None,\n",
        "    id=None\n",
        "):\n",
        "  '''linear probe eval with logistic regression model'''\n",
        "  use_wandb = project is not None and id is not None\n",
        "  if use_wandb:\n",
        "    wandb.init(\n",
        "        project=project,\n",
        "        # entity=,\n",
        "        id=id,\n",
        "        resume='allow'\n",
        "    )\n",
        "  pl.seed_everything(seed)\n",
        "  dm = dm_init(**dm_kwargs)\n",
        "  encoder = model_preprocess(deepcopy(model))\n",
        "  lit_encoder = LitModel(\n",
        "      encoder, 10, param_fn=lambda model: model[-1].parameters()\n",
        "  )\n",
        "  trainer = create_trainer(strategy='dp' if not COLAB else 'auto')\n",
        "  dm.prepare_data()\n",
        "  dm.setup()\n",
        "  preds = trainer.predict(lit_encoder, dm.train_dataloader())\n",
        "  print(preds)\n",
        "  embs = torch.cat([pred['logits'] for pred in preds])\n",
        "  labels = torch.cat([pred['y'] for pred in preds])\n",
        "  log_reg = (\n",
        "      LogisticRegression(max_iter=5000, random_state=seed, verbose=True, **sk_kwargs)\n",
        "      .fit(embs, labels)\n",
        "  )\n",
        "  val_preds = trainer.predict(lit_encoder, dm.val_dataloader())\n",
        "  val_embs = torch.cat([pred['logits'] for pred in val_preds])\n",
        "  val_labels = torch.cat([pred['y'] for pred in val_preds])\n",
        "  val_acc = log_reg.score(val_embs, val_labels)\n",
        "  print(val_acc)\n",
        "  if use_wandb:\n",
        "    wandb.log({'val/acc': val_acc})\n",
        "  return log_reg\n",
        "\n",
        "def quick_fn_probe(\n",
        "    quick_fn,\n",
        "    probe_fn,\n",
        "    fn_kwargs={},\n",
        "    probe_kwargs={},\n",
        "    extract_model_fn=lambda lit_model: lit_model.model,\n",
        "):\n",
        "  lit_model = quick_fn(**fn_kwargs)\n",
        "  if fn_kwargs.get('project') and fn_kwargs.get('name'):\n",
        "    probe_kwargs.update({'project': fn_kwargs['project'], 'id': wandb.run.id})\n",
        "  lit_linear = probe_fn(extract_model_fn(lit_model), **probe_kwargs)\n",
        "  return lit_model, lit_linear\n",
        "\n",
        "def quick_sk_test(\n",
        "    model,\n",
        "    linear,\n",
        "    dm_init,\n",
        "    dm_kwargs={},\n",
        "    model_preprocess=lambda model: model,\n",
        "    project=None,\n",
        "    id=None\n",
        "):\n",
        "  '''test function for logistic regression model'''\n",
        "  use_wandb = project is not None and id is not None\n",
        "  if use_wandb:\n",
        "    wandb.init(\n",
        "        project=project,\n",
        "        # entity=,\n",
        "        id=id,\n",
        "        resume='allow'\n",
        "    )\n",
        "  dm = dm_init(**dm_kwargs)\n",
        "  encoder = deepcopy(model_preprocess(model))\n",
        "  lit_encoder = LitModel(encoder, 10) # don't worry, final dim is not 10\n",
        "  trainer = create_trainer()\n",
        "  dm.prepare_data()\n",
        "  dm.setup()\n",
        "  test_preds = trainer.predict(lit_encoder, dm.test_dataloader())\n",
        "  test_embs = torch.cat([pred['logits'] for pred in test_preds])\n",
        "  test_labels = torch.cat([pred['y'] for pred in test_preds])\n",
        "  test_acc = linear.score(test_embs, test_labels)\n",
        "  print(test_acc)\n",
        "  if use_wandb:\n",
        "    wandb.log({'test/acc': test_acc})\n",
        "  return test_acc\n",
        "\n",
        "def quick_lit_test(\n",
        "    lit_model,\n",
        "    dm_init,\n",
        "    dm_kwargs,\n",
        "    model_preprocess=lambda model: model,\n",
        "    project=None,\n",
        "    id=None,\n",
        "):\n",
        "  '''test function for model'''\n",
        "  use_wandb = project is not None and id is not None\n",
        "  trainer_kwargs = {}\n",
        "  if use_wandb:\n",
        "    wandb.init(\n",
        "        project=project,\n",
        "        # entity=,\n",
        "        id=id,\n",
        "        resume='allow'\n",
        "    )\n",
        "  dm = dm_init(**dm_kwargs)\n",
        "  trainer = create_trainer(**trainer_kwargs)\n",
        "  test_acc = trainer.test(lit_model, dm)[0]['test/acc']\n",
        "  if use_wandb:\n",
        "    wandb.log({'test/acc': test_acc})\n",
        "  print(test_acc)\n",
        "  return test_acc"
      ],
      "metadata": {
        "id": "8TahPXx23S9k",
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2023-05-01T06:57:15.426304Z",
          "iopub.execute_input": "2023-05-01T06:57:15.426665Z",
          "iopub.status.idle": "2023-05-01T06:57:15.460754Z",
          "shell.execute_reply.started": "2023-05-01T06:57:15.426630Z",
          "shell.execute_reply": "2023-05-01T06:57:15.459115Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_student = lambda lit_model: lit_model.student"
      ],
      "metadata": {
        "id": "WQ2r-lGYqRhA",
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2023-05-01T06:57:15.771799Z",
          "iopub.execute_input": "2023-05-01T06:57:15.772186Z",
          "iopub.status.idle": "2023-05-01T06:57:15.779256Z",
          "shell.execute_reply.started": "2023-05-01T06:57:15.772150Z",
          "shell.execute_reply": "2023-05-01T06:57:15.777843Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def strip_resnet_cls_head(resnet):\n",
        "  return nn.Sequential(\n",
        "      *list(resnet.children())[:-1],\n",
        "      nn.AdaptiveAvgPool2d(1),\n",
        "      nn.Flatten()\n",
        "  )"
      ],
      "metadata": {
        "id": "uWMVGc6IMV4z",
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2023-05-01T06:57:16.042112Z",
          "iopub.execute_input": "2023-05-01T06:57:16.042551Z",
          "iopub.status.idle": "2023-05-01T06:57:16.048592Z",
          "shell.execute_reply.started": "2023-05-01T06:57:16.042517Z",
          "shell.execute_reply": "2023-05-01T06:57:16.047099Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def experiment(\n",
        "    seeds,\n",
        "    teacher_kwargs,\n",
        "    small_kwargs,\n",
        "    distilled_kwargs,\n",
        "    extract_teacher_fn,\n",
        "    test_dm_init,\n",
        "    test_dm_kwargs={},\n",
        "    test_fn=None,\n",
        "    project=None\n",
        "):\n",
        "  # initialize empty results\n",
        "  teacher_results = []\n",
        "  small_results = []\n",
        "  distilled_results = []\n",
        "\n",
        "  # dm and trainer for testing just in case\n",
        "  dm = test_dm_init(**test_dm_kwargs)\n",
        "  dm.prepare_data()\n",
        "  dm.setup()\n",
        "  trainer = create_trainer()\n",
        "\n",
        "  # set wandb details\n",
        "  # if project is `None`, no logging anyway, so safe to input experiment\n",
        "  for kwargs, name in zip(\n",
        "      (teacher_kwargs, small_kwargs, distilled_kwargs),\n",
        "      ('teacher', 'baseline', 'student')\n",
        "  ):\n",
        "    if 'fn_kwargs' in kwargs:\n",
        "      kwargs = kwargs['fn_kwargs']\n",
        "    kwargs.update({'project': project, 'name': name})\n",
        "\n",
        "  # one trial for each seed\n",
        "  for seed in seeds:\n",
        "\n",
        "    # teacher\n",
        "    if teacher_kwargs.get('quick_fn') is not None:\n",
        "      # ssl: train and probe\n",
        "      teacher_kwargs['fn_kwargs'].update({'seed': seed})\n",
        "      if teacher_kwargs.get('probe_kwargs') is None:\n",
        "        teacher_kwargs['probe_kwargs'] = {'seed': seed}\n",
        "      else:\n",
        "        teacher_kwargs['probe_kwargs'].update({'seed': seed})\n",
        "      lit_teacher, teacher_linear = quick_fn_probe(**teacher_kwargs)\n",
        "      if isinstance(teacher_linear, LogisticRegression):\n",
        "        results = test_fn(\n",
        "            teacher_kwargs.get(\n",
        "                'extract_model_fn', lambda lit_model: lit_model.model\n",
        "            )(lit_teacher),\n",
        "            teacher_linear\n",
        "        )\n",
        "      else:\n",
        "        pass # not using fc probe rn anyway\n",
        "    else:\n",
        "      # sl: just train\n",
        "      lit_teacher = quick_train(**teacher_kwargs, seed=seed)\n",
        "      results = trainer.test(lit_teacher, dm)[0]['test/acc']\n",
        "    teacher_results.append(results)\n",
        "\n",
        "    # student (no distillation)\n",
        "    if small_kwargs.get('quick_fn') is not None:\n",
        "      # ssl: train and probe\n",
        "      small_kwargs['fn_kwargs'].update({'seed': seed})\n",
        "      if small_kwargs.get('probe_kwargs') is None:\n",
        "        small_kwargs['probe_kwargs'] = {'seed': seed}\n",
        "      else:\n",
        "        small_kwargs['probe_kwargs'].update({'seed': seed})\n",
        "      lit_small, small_linear = quick_fn_probe(**small_kwargs)\n",
        "      if isinstance(small_linear, LogisticRegression):\n",
        "        results = test_fn(\n",
        "            small_kwargs.get(\n",
        "                'extract_model_fn', lambda lit_model: lit_model.model\n",
        "            )(lit_small),\n",
        "            small_linear\n",
        "        )\n",
        "      else:\n",
        "        pass # not using fc probe rn anyway\n",
        "    else:\n",
        "      # sl: just train\n",
        "      lit_small = quick_train(**small_kwargs, seed=seed)\n",
        "      results = trainer.test(lit_small, dm)[0]['test/acc']\n",
        "    small_results.append(results)\n",
        "\n",
        "    # student (with distillation)\n",
        "    # always train and probe\n",
        "    distilled_kwargs['fn_kwargs'].update({'seed': seed})\n",
        "    distilled_kwargs['fn_kwargs']['distill_kwargs'].update({'teacher': extract_teacher_fn(lit_teacher)})\n",
        "    if distilled_kwargs.get('probe_kwargs') is None:\n",
        "      distilled_kwargs['probe_kwargs'] = {'seed': seed}\n",
        "    else:\n",
        "      distilled_kwargs['probe_kwargs'].update({'seed': seed})\n",
        "    lit_distilled, distilled_linear = quick_fn_probe(**distilled_kwargs)\n",
        "    if isinstance(distilled_linear, LogisticRegression):\n",
        "      results = test_fn(lit_distilled.student, distilled_linear)\n",
        "    else:\n",
        "      pass # not using fc probe rn anyway\n",
        "    distilled_results.append(results)\n",
        "\n",
        "  param_counts = [\n",
        "      param_count(model)\n",
        "      for model\n",
        "      in (\n",
        "          lit_distilled.teacher,\n",
        "          distilled_kwargs['quick_fn'].keywords.get(\n",
        "              'student_preprocess',\n",
        "              distilled_kwargs['fn_kwargs'].get(\n",
        "                  'student_preprocess',\n",
        "                  lambda model: model\n",
        "              )\n",
        "          )(small_kwargs.get('extract_model_fn', lambda lit_model: lit_model.model)(lit_small)),\n",
        "          lit_distilled.student\n",
        "      )\n",
        "  ]\n",
        "\n",
        "  return param_counts, teacher_results, small_results, distilled_results"
      ],
      "metadata": {
        "id": "0o7OvoXyTPFE",
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2023-05-01T06:57:16.412358Z",
          "iopub.execute_input": "2023-05-01T06:57:16.412790Z",
          "iopub.status.idle": "2023-05-01T06:57:16.433253Z",
          "shell.execute_reply.started": "2023-05-01T06:57:16.412756Z",
          "shell.execute_reply": "2023-05-01T06:57:16.432002Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utils"
      ],
      "metadata": {
        "id": "q8O-hrBWvaDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def same_params(model_0, model_1):\n",
        "  return all([(p_0 == p_1).all() for (p_0, p_1) in zip(model_0.parameters(), model_1.parameters())])"
      ],
      "metadata": {
        "id": "RkYjtytq03f3",
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2023-05-01T06:57:17.212183Z",
          "iopub.execute_input": "2023-05-01T06:57:17.212556Z",
          "iopub.status.idle": "2023-05-01T06:57:17.218425Z",
          "shell.execute_reply.started": "2023-05-01T06:57:17.212521Z",
          "shell.execute_reply": "2023-05-01T06:57:17.217215Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def param_count(model):\n",
        "  return sum([p.numel() for p in model.parameters()])"
      ],
      "metadata": {
        "id": "l-qykHNm0Qmu",
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2023-05-01T06:57:17.574048Z",
          "iopub.execute_input": "2023-05-01T06:57:17.574787Z",
          "iopub.status.idle": "2023-05-01T06:57:17.580644Z",
          "shell.execute_reply.started": "2023-05-01T06:57:17.574745Z",
          "shell.execute_reply": "2023-05-01T06:57:17.578977Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def extract_embeddings(models, dataloader):\n",
        "  if isinstance(models, nn.Module):\n",
        "    models = [models]\n",
        "  embeddings = []\n",
        "  trainer = create_trainer()\n",
        "  for model in models:\n",
        "    embeddings.append(torch.cat([\n",
        "        output['logits']\n",
        "        for output in trainer.predict(LitModel(model, num_classes=10), dataloader) # ignore `num_classes`\n",
        "    ]))\n",
        "  return embeddings"
      ],
      "metadata": {
        "id": "go39X3h-0qq7",
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2023-05-01T06:57:17.970933Z",
          "iopub.execute_input": "2023-05-01T06:57:17.971531Z",
          "iopub.status.idle": "2023-05-01T06:57:17.978402Z",
          "shell.execute_reply.started": "2023-05-01T06:57:17.971486Z",
          "shell.execute_reply": "2023-05-01T06:57:17.977176Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_cos_sim(model_0, model_1, dataloader):\n",
        "  all_logits_0, all_logits_1 = extract_embeddings(\n",
        "      [model_0, model_1], dataloader\n",
        "  )\n",
        "\n",
        "  all_logits_0 = all_logits_0 / all_logits_0.norm(dim=1, keepdim=True)\n",
        "  all_logits_1 = all_logits_1 / all_logits_1.norm(dim=1, keepdim=True)\n",
        "\n",
        "  all_logits_0 = all_logits_0.mm(all_logits_0.T)\n",
        "  all_logits_1 = all_logits_1.mm(all_logits_1.T)\n",
        "\n",
        "  cos = F.cosine_similarity(all_logits_0, all_logits_1).mean().item()\n",
        "  return cos"
      ],
      "metadata": {
        "id": "0Pgx3ihFsHd8",
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2023-05-01T06:57:18.401155Z",
          "iopub.execute_input": "2023-05-01T06:57:18.402479Z",
          "iopub.status.idle": "2023-05-01T06:57:18.409486Z",
          "shell.execute_reply.started": "2023-05-01T06:57:18.402433Z",
          "shell.execute_reply": "2023-05-01T06:57:18.408196Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiments"
      ],
      "metadata": {
        "id": "UfNNS2UjQ9Oj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Self-Supervised"
      ],
      "metadata": {
        "id": "VYPAw_5WE_sx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### MNIST"
      ],
      "metadata": {
        "id": "bUcv9XATFSSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_ssl_quick_train = partial(\n",
        "    quick_train,\n",
        "    lit_model_cls=LitAE,\n",
        "    dm_init=GenericDataModule,\n",
        "    dm_kwargs={'ds_class': MNIST, 'batch_size': 128},\n",
        "    trainer_kwargs={'max_epochs': 20}\n",
        ")\n",
        "\n",
        "mnist_ssl_quick_distill = partial(\n",
        "    quick_distill,\n",
        "    student_init=create_mnist_ae_student,\n",
        "    dm_init=GenericDataModule,\n",
        "    dm_kwargs={'ds_class': MNIST, 'batch_size': 128},\n",
        "    student_preprocess=lambda model: model[:5],\n",
        "    trainer_kwargs={'max_epochs': 20}\n",
        ")\n",
        "\n",
        "mnist_ssl_quick_sk_probe = partial(\n",
        "    quick_sk_probe,\n",
        "    dm_init=GenericDataModule,\n",
        "    dm_kwargs=mnist_1024,\n",
        "    model_preprocess=lambda model: model[:5],\n",
        ")\n",
        "\n",
        "mnist_ssl_sk_test = partial(\n",
        "    quick_sk_test,\n",
        "    dm_init=GenericDataModule,\n",
        "    dm_kwargs=mnist_1024,\n",
        "    model_preprocess=lambda model: model[:5]\n",
        ")"
      ],
      "metadata": {
        "id": "cWzrYZ7q3aIO",
        "scrolled": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lit_teacher, teacher_linear = quick_fn_probe(\n",
        "    mnist_ssl_quick_train,\n",
        "    mnist_ssl_quick_sk_probe,\n",
        "    {'model_init': create_mnist_ae_teacher, 'lit_model_kwargs': {'lr': 1e-1}}\n",
        ")"
      ],
      "metadata": {
        "id": "7GBGTDCN96cn",
        "scrolled": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lit_teacher, teacher_linear = quick_fn_probe(\n",
        "    mnist_ssl_quick_train,\n",
        "    mnist_ssl_quick_sk_probe,\n",
        "    {'model_init': create_mnist_ae_teacher, 'lit_model_kwargs': {'lr': 1e-1}, 'project': 'rel-rep-dist-mnist-ssl-viz', 'name': 'teacher', 'log_model': True}\n",
        ")"
      ],
      "metadata": {
        "id": "A8hCr3An70P5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lit_student, student_linear = quick_fn_probe(\n",
        "    mnist_ssl_quick_train,\n",
        "    mnist_ssl_quick_sk_probe,\n",
        "    {'model_init': create_mnist_ae_student, 'lit_model_kwargs': {'lr': 1e-8}, 'project': 'rel-rep-dist-mnist-ssl-viz', 'name': 'baseline', 'log_model': True}\n",
        ")"
      ],
      "metadata": {
        "id": "M-rArEb12qhe",
        "scrolled": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lit_distilled, distilled_linear = quick_fn_probe(\n",
        "    mnist_ssl_quick_distill,\n",
        "    mnist_ssl_quick_sk_probe,\n",
        "    {'distill_kwargs': {'teacher': lit_teacher.model[:5], 'lr': 1e-1},  'project': 'rel-rep-dist-mnist-ssl-viz', 'name': 'student', 'log_model': True},\n",
        "    extract_model_fn=extract_student\n",
        ")"
      ],
      "metadata": {
        "id": "pQqXz95N9lHJ",
        "scrolled": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_counts, teacher_results, small_results, distilled_results = experiment(\n",
        "    seeds=[42, 43, 44],\n",
        "    teacher_kwargs={\n",
        "        'quick_fn': mnist_ssl_quick_train,\n",
        "        'probe_fn': mnist_ssl_quick_sk_probe,\n",
        "        'fn_kwargs': {'model_init': create_mnist_ae_teacher, 'lit_model_kwargs': {'lr': 1e-1}}\n",
        "    },\n",
        "    small_kwargs={\n",
        "        'quick_fn': mnist_ssl_quick_train,\n",
        "        'probe_fn': mnist_ssl_quick_sk_probe,\n",
        "        'fn_kwargs': {'model_init': create_mnist_ae_student, 'lit_model_kwargs': {'lr': 1e-8}}\n",
        "    },\n",
        "    distilled_kwargs={\n",
        "        'quick_fn': mnist_ssl_quick_distill,\n",
        "        'probe_fn': mnist_ssl_quick_sk_probe,\n",
        "        'fn_kwargs': {'distill_kwargs': {'lr': 1e-1}},\n",
        "        'extract_model_fn': extract_student\n",
        "    },\n",
        "    extract_teacher_fn=lambda lit_teacher: lit_teacher.model[:5],\n",
        "    test_dm_init=GenericDataModule,\n",
        "    test_dm_kwargs=mnist_1024,\n",
        "    test_fn=mnist_ssl_sk_test,\n",
        "    project='rel-rep-dist-mnist-ssl'\n",
        ")"
      ],
      "metadata": {
        "id": "jAMF6bAI4cck",
        "scrolled": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# param count vs test acc\n",
        "accs = [np.mean([results]) for results in (teacher_results, small_results, distilled_results)]\n",
        "\n",
        "labels = ['AE-64', 'AE-32 (no distillation)', 'AE-32 (with distillation)']\n",
        "for count, acc, label in zip(param_counts, accs, labels):\n",
        "  plt.scatter(count, acc, label=label)\n",
        "plt.xlabel('parameter count')\n",
        "plt.ylabel('test accuracy')\n",
        "plt.legend()\n",
        "plt.title('MNIST')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yV9rdu-hvkdc",
        "scrolled": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CIFAR10"
      ],
      "metadata": {
        "id": "3y6wXGoXtDQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cifar10_ssl_quick_train = partial(\n",
        "    quick_train,\n",
        "    lit_model_cls=SimCLR,\n",
        "    dm_init=SimCLRCIFAR10DataModule,\n",
        "    dm_kwargs={'batch_size': 256},\n",
        "    trainer_kwargs={'max_epochs': 10}\n",
        ")\n",
        "\n",
        "cifar10_ssl_quick_distill = partial(\n",
        "    quick_distill,\n",
        "    student_init=create_cifar10_simclr_student,\n",
        "    dm_init=GenericDataModule,\n",
        "    dm_kwargs=cifar10_64,\n",
        "    trainer_kwargs={'max_epochs': 10}\n",
        ")\n",
        "\n",
        "cifar10_ssl_quick_sk_probe = partial(\n",
        "    quick_sk_probe,\n",
        "    dm_init=GenericDataModule,\n",
        "    dm_kwargs=cifar10_1024,\n",
        ")\n",
        "\n",
        "cifar10_ssl_sk_test = partial(\n",
        "    quick_sk_test,\n",
        "    dm_init=GenericDataModule,\n",
        "    dm_kwargs=cifar10_1024,\n",
        ")"
      ],
      "metadata": {
        "id": "PWAidDsGp0_c",
        "scrolled": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_counts, teacher_results, small_results, distilled_results = experiment(\n",
        "    seeds=[42, 43, 44],\n",
        "    teacher_kwargs={\n",
        "        'quick_fn': cifar10_ssl_quick_train,\n",
        "        'probe_fn': cifar10_ssl_quick_sk_probe,\n",
        "        'fn_kwargs': {'model_init': create_cifar10_simclr_teacher, 'lit_model_kwargs': {'lr': 1e-1}, 'log_model': True},\n",
        "        'extract_model_fn': lambda lit_model: lit_model.backbone\n",
        "    },\n",
        "    small_kwargs={\n",
        "        'quick_fn': cifar10_ssl_quick_train,\n",
        "        'probe_fn': cifar10_ssl_quick_sk_probe,\n",
        "        'fn_kwargs': {'model_init': create_cifar10_simclr_student, 'lit_model_kwargs': {'lr': 1e-1}, 'trainer_kwargs': {'max_steps': 10}},\n",
        "        'extract_model_fn': lambda lit_model: lit_model.backbone\n",
        "    },\n",
        "    distilled_kwargs={\n",
        "        'quick_fn': cifar10_ssl_quick_distill,\n",
        "        'probe_fn': cifar10_ssl_quick_sk_probe,\n",
        "        'fn_kwargs': {'distill_kwargs': {'lr': 1e-1}, 'log_model': True},\n",
        "        'extract_model_fn': extract_student\n",
        "    },\n",
        "    extract_teacher_fn=lambda lit_teacher: lit_teacher.backbone,\n",
        "    test_dm_init=GenericDataModule,\n",
        "    test_dm_kwargs=cifar10_1024,\n",
        "    test_fn=cifar10_ssl_sk_test,\n",
        "    project='rel-rep-dist-cifar-ssl-tune'\n",
        ")"
      ],
      "metadata": {
        "id": "EXstSBWbZVP_",
        "scrolled": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# param count vs test acc\n",
        "accs = [np.mean([results]) for results in (teacher_results, small_results, distilled_results)]\n",
        "\n",
        "labels = ['ResNet-18', 'ResNet-9×0.5 (no distillation)', 'ResNet-9×0.5 (with distillation)']\n",
        "for count, acc, label in zip(param_counts, accs, labels):\n",
        "  plt.scatter(count, acc, label=label)\n",
        "plt.xlabel('parameter count')\n",
        "plt.ylabel('test accuracy')\n",
        "plt.legend()\n",
        "plt.title('CIFAR10')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xSZFdHg6Paw5",
        "scrolled": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Supervised"
      ],
      "metadata": {
        "id": "oMCM_35oFDpW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### MNIST"
      ],
      "metadata": {
        "id": "QewDQSC3MHAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_sl_quick_train = partial(\n",
        "    quick_train,\n",
        "    lit_model_cls=LitModel,\n",
        "    dm_init=GenericDataModule,\n",
        "    dm_kwargs={'ds_class': MNIST, 'batch_size': 128},\n",
        "    trainer_kwargs={'max_epochs': 20}\n",
        ")\n",
        "\n",
        "mnist_sl_quick_distill = partial(\n",
        "    quick_distill,\n",
        "    student_init=create_mnist_cls_student,\n",
        "    dm_init=GenericDataModule,\n",
        "    dm_kwargs={'ds_class': MNIST, 'batch_size': 128},\n",
        "    trainer_kwargs={'max_epochs': 20},\n",
        "    student_preprocess=lambda model: model[:5],\n",
        ")\n",
        "\n",
        "mnist_sl_quick_sk_probe = partial(\n",
        "    quick_sk_probe,\n",
        "    dm_init=GenericDataModule,\n",
        "    dm_kwargs=mnist_1024,\n",
        "    model_preprocess=lambda model: model[:5],\n",
        ")\n",
        "\n",
        "mnist_sl_quick_fc_probe = partial(\n",
        "    quick_fc_probe,\n",
        "    dm_init=GenericDataModule,\n",
        "    dm_kwargs=mnist_1024,\n",
        "    model_preprocess=lambda model: model[:5],\n",
        ")\n",
        "\n",
        "mnist_sl_sk_test = partial(\n",
        "    quick_sk_test,\n",
        "    dm_init=GenericDataModule,\n",
        "    dm_kwargs=mnist_1024,\n",
        "    model_preprocess=lambda model: model[:5]\n",
        ")"
      ],
      "metadata": {
        "id": "2orc926PRuJn",
        "scrolled": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_counts, teacher_results, small_results, distilled_results = experiment(\n",
        "    seeds=[42, 43, 44],\n",
        "    teacher_kwargs={\n",
        "        'model_init': create_mnist_cls_teacher,\n",
        "        'lit_model_kwargs': {'num_classes': 10, 'lr': 1e-1},\n",
        "        **mnist_sl_quick_train.keywords\n",
        "    },\n",
        "    small_kwargs={\n",
        "        'model_init': create_mnist_cls_student,\n",
        "        'lit_model_kwargs': {'num_classes': 10, 'lr': 1e-1},\n",
        "        **mnist_sl_quick_train.keywords\n",
        "    },\n",
        "    distilled_kwargs={\n",
        "        'quick_fn': mnist_sl_quick_distill,\n",
        "        'probe_fn': mnist_sl_quick_sk_probe,\n",
        "        'fn_kwargs': {\n",
        "          'distill_kwargs': {\n",
        "              'lr': 1e-1,\n",
        "              'd_weight': 1,\n",
        "              'dim': 32,\n",
        "              'num_classes': 10,\n",
        "              'ce_weight': 1,\n",
        "              'non_linear_head': True,\n",
        "              'dropout_head': True\n",
        "          }\n",
        "        },\n",
        "        'extract_model_fn': extract_student\n",
        "    },\n",
        "    extract_teacher_fn=lambda lit_teacher: lit_teacher.model[:5],\n",
        "    test_dm_init=GenericDataModule,\n",
        "    test_dm_kwargs=mnist_1024,\n",
        "    test_fn=mnist_sl_sk_test,\n",
        "    project='rel-rep-dist-mnist-sl'\n",
        ")"
      ],
      "metadata": {
        "id": "0-IfM89h3BMk",
        "scrolled": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# param count vs test acc\n",
        "accs = [np.mean(results) for results in (teacher_results, small_results, distilled_results)]\n",
        "\n",
        "labels = ['MLP-64', 'MLP-32 (no distillation)', 'MLP-32 (with distillation)']\n",
        "for count, acc, label in zip([param_counts[i] for i in (0, 1, 1)], accs, labels):\n",
        "  plt.scatter(count, acc, label=label)\n",
        "plt.xlabel('parameter count')\n",
        "plt.ylabel('test accuracy')\n",
        "plt.legend()\n",
        "plt.title('MNIST')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bR7l6t9NVXzJ",
        "scrolled": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CIFAR10"
      ],
      "metadata": {
        "id": "itcsjJG64rlu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cifar10_sl_quick_train = partial(\n",
        "    quick_train,\n",
        "    lit_model_cls=LitModel,\n",
        "    dm_init=GenericDataModule,\n",
        "    dm_kwargs={'ds_class': CIFAR10, 'batch_size': 128},\n",
        "    trainer_kwargs={'max_epochs': 10}\n",
        ")\n",
        "\n",
        "cifar10_sl_quick_distill = partial(\n",
        "    quick_distill,\n",
        "    student_init=create_cifar10_cls_student,\n",
        "    dm_init=GenericDataModule,\n",
        "    dm_kwargs={'ds_class': CIFAR10, 'batch_size': 128},\n",
        "    trainer_kwargs={'max_epochs': 10},\n",
        "    student_preprocess=strip_resnet_cls_head\n",
        ")\n",
        "\n",
        "cifar10_sl_quick_sk_probe = partial(\n",
        "    quick_sk_probe,\n",
        "    dm_init=GenericDataModule,\n",
        "    dm_kwargs=cifar10_1024,\n",
        ")\n",
        "\n",
        "cifar10_sl_quick_fc_probe = partial(\n",
        "    quick_fc_probe,\n",
        "    dm_init=GenericDataModule,\n",
        "    dm_kwargs=cifar10_1024,\n",
        ")\n",
        "\n",
        "cifar10_sl_sk_test = partial(\n",
        "    quick_sk_test,\n",
        "    dm_init=GenericDataModule,\n",
        "    dm_kwargs=cifar10_1024,\n",
        ")"
      ],
      "metadata": {
        "id": "P-tPz28_Qihf",
        "scrolled": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_counts, teacher_results, small_results, distilled_results = experiment(\n",
        "    seeds=[42, 43, 44],\n",
        "    teacher_kwargs={\n",
        "        'model_init': create_cifar10_cls_teacher,\n",
        "        'lit_model_kwargs': {'lr': 1e-1, 'num_classes': 10},\n",
        "        **cifar10_sl_quick_train.keywords\n",
        "    },\n",
        "    small_kwargs={\n",
        "        'model_init': create_cifar10_cls_student,\n",
        "        'lit_model_kwargs': {'lr': 1e-1, 'num_classes': 10},\n",
        "        **cifar10_sl_quick_train.keywords\n",
        "    },\n",
        "    distilled_kwargs={\n",
        "        'quick_fn': cifar10_sl_quick_distill,\n",
        "        'probe_fn': cifar10_sl_quick_sk_probe,\n",
        "        'fn_kwargs': {\n",
        "            'distill_kwargs': {\n",
        "                'lr': 1e-1,\n",
        "                'd_weight': 1,\n",
        "                'dim': 256,\n",
        "                'num_classes': 10,\n",
        "                'ce_weight': 1,\n",
        "                'non_linear_head': False,\n",
        "                'dropout_head': False\n",
        "            }\n",
        "        },\n",
        "        'extract_model_fn': extract_student\n",
        "    },\n",
        "    extract_teacher_fn=lambda lit_teacher: nn.Sequential(*list(lit_teacher.model.children())[:-1], nn.AdaptiveAvgPool2d(1), nn.Flatten()),\n",
        "    test_dm_init=GenericDataModule,\n",
        "    test_dm_kwargs=cifar10_1024,\n",
        "    test_fn=cifar10_sl_sk_test,\n",
        "    project='rel-rep-dist-cifar-sl'\n",
        ")"
      ],
      "metadata": {
        "id": "WuiXV5I7m2zZ",
        "scrolled": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# param count vs test acc\n",
        "accs = [np.mean(results) for results in (teacher_results, small_results, distilled_results)]\n",
        "\n",
        "labels = ['ResNet-18', 'ResNet-9×0.5 (no distillation)', 'ResNet-9×0.5 (with distillation)']\n",
        "for count, acc, label in zip(param_counts, accs, labels):\n",
        "  plt.scatter(count, acc, label=label)\n",
        "plt.xlabel('parameter count')\n",
        "plt.ylabel('test accuracy')\n",
        "plt.legend()\n",
        "plt.title('CIFAR10')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pFESAePrvtkv",
        "scrolled": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Anchor selection"
      ],
      "metadata": {
        "id": "SfKkgHnzfP8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dm = GenericDataModule(MNIST, batch_size=128)\n",
        "dm.prepare_data()\n",
        "dm.setup()"
      ],
      "metadata": {
        "id": "ZTIqGMSTidI5",
        "scrolled": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anc_sel_quick_train = partial(\n",
        "    quick_train,\n",
        "    lit_model_cls=LitAE,\n",
        "    dm_init=GenericDataModule,\n",
        "    dm_kwargs={'ds_class': MNIST, 'batch_size': 128},\n",
        "    trainer_kwargs={'max_epochs': 20}\n",
        ")\n",
        "\n",
        "anc_sel_quick_distill = partial(\n",
        "    quick_distill,\n",
        "    student_init=create_mnist_ae_student,\n",
        "    dm_init=GenericDataModule,\n",
        "    dm_kwargs={'ds_class': MNIST, 'batch_size': 128},\n",
        "    student_preprocess=lambda model: model[:5],\n",
        "    trainer_kwargs={'max_epochs': 20}\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "efxlBo-iKsTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def anchor_experiment(seeds, project=None):\n",
        "  distilled_results = []\n",
        "  distilled_rand_results = []\n",
        "  distilled_per_class_results = []\n",
        "  distilled_best_per_class_results = []\n",
        "\n",
        "  dm = GenericDataModule(MNIST, batch_size=128)\n",
        "  dm.prepare_data()\n",
        "  dm.setup()\n",
        "\n",
        "  for seed in seeds:\n",
        "    # teacher\n",
        "    lit_teacher, teacher_linear = quick_fn_probe(\n",
        "        anc_sel_quick_train,\n",
        "        mnist_ssl_quick_sk_probe,\n",
        "        {'model_init': create_mnist_ae_teacher, 'lit_model_kwargs': {'lr': 1e-1}, 'seed': seed, 'project': project, 'name': 'teacher'},\n",
        "        {'seed': seed}\n",
        "    )\n",
        "\n",
        "    # in batch\n",
        "    lit_distilled, distilled_linear = quick_fn_probe(\n",
        "        anc_sel_quick_distill,\n",
        "        mnist_ssl_quick_sk_probe,\n",
        "        {'distill_kwargs': {'teacher': lit_teacher.model[:5], 'lr': 1e-1},  'seed': seed, 'project': project, 'name': 'in-batch'},\n",
        "        {'seed': seed},\n",
        "        extract_model_fn=lambda lit_model: lit_model.student\n",
        "    )\n",
        "    id = {'id': wandb.run.id} if project is not None else {}\n",
        "    distilled_results.append(mnist_ssl_sk_test(lit_distilled.student, distilled_linear, project=project, **id))\n",
        "\n",
        "    # rand\n",
        "    anchors, _ = random_split(dm.ds_train, [128, len(dm.ds_train) - 128], generator=torch.Generator().manual_seed(seed))\n",
        "    anchors = torch.stack([a for (a, _) in anchors]) # iteration, yeah, but it's only 128 items and done once\n",
        "\n",
        "    lit_distilled_rand, distilled_rand_linear = quick_fn_probe(\n",
        "        anc_sel_quick_distill,\n",
        "        mnist_ssl_quick_sk_probe,\n",
        "        {'distill_kwargs': {'teacher': lit_teacher.model[:5], 'lr': 1e-1, 'anchors': anchors}, 'seed': seed, 'project': project, 'name': 'random'},\n",
        "        {'seed': seed},\n",
        "        extract_model_fn=lambda lit_model: lit_model.student\n",
        "    )\n",
        "    id = {'id': wandb.run.id} if project is not None else {}\n",
        "    distilled_rand_results.append(mnist_ssl_sk_test(lit_distilled_rand.student, distilled_rand_linear, project=project, **id))\n",
        "\n",
        "    # per class\n",
        "    ds_train = dm.ds_train\n",
        "    sorter = torch.randperm(len(ds_train), generator=torch.Generator().manual_seed(seed))\n",
        "    ds_train_data = ds_train.dataset.data[ds_train.indices][sorter]\n",
        "    ds_train_labels = ds_train.dataset.targets[ds_train.indices][sorter]\n",
        "    anchors = torch.cat([ds_train_data[ds_train_labels == i][:13] for i in range(10)])\n",
        "    anchors = ((anchors / 255 - 0.5) / 0.5).unsqueeze(1)\n",
        "\n",
        "    lit_distilled_per_class, distilled_per_class_linear = quick_fn_probe(\n",
        "        anc_sel_quick_distill,\n",
        "        mnist_ssl_quick_sk_probe,\n",
        "        {'distill_kwargs': {'teacher': lit_teacher.model[:5], 'lr': 1e-1, 'anchors': anchors}, 'seed': seed, 'project': project, 'name': 'random-per-class'},\n",
        "        {'seed': seed},\n",
        "        extract_model_fn=lambda lit_model: lit_model.student\n",
        "    )\n",
        "    id = {'id': wandb.run.id} if project is not None else {}\n",
        "    distilled_per_class_results.append(mnist_ssl_sk_test(lit_distilled_per_class.student, distilled_per_class_linear, project=project, **id))\n",
        "\n",
        "    # best per class\n",
        "    preds = create_trainer().predict(\n",
        "        LitModel(deepcopy(lit_teacher.model[:5]), 10),\n",
        "        dm.train_dataloader()\n",
        "    )\n",
        "    logits = torch.tensor(teacher_linear.decision_function(torch.cat([pred['logits'] for pred in preds]))).softmax(dim=-1)\n",
        "    labels = torch.cat([pred['y'] for pred in preds])\n",
        "    pred_labels = logits.argmax(dim=-1)\n",
        "    correct_pred_filter = (pred_labels == labels)\n",
        "\n",
        "    ds_train = dm.ds_train\n",
        "    ds_train_data = ds_train.dataset.data[ds_train.indices]\n",
        "    ds_train_labels = ds_train.dataset.targets[ds_train.indices]\n",
        "\n",
        "    anchors = torch.cat([\n",
        "        ds_train_data[correct_pred_filter & (labels == i)] \\\n",
        "        [logits[correct_pred_filter & (labels == i)][:, i].argsort(descending=True)[:13]]\n",
        "        for i in range(10)\n",
        "    ])\n",
        "    anchors = ((anchors / 255 - 0.5) / 0.5).unsqueeze(1)\n",
        "\n",
        "    lit_distilled_best_per_class, distilled_best_per_class_linear = quick_fn_probe(\n",
        "        anc_sel_quick_distill,\n",
        "        mnist_ssl_quick_sk_probe,\n",
        "        {'distill_kwargs': {'teacher': lit_teacher.model[:5], 'lr': 1e-1, 'anchors': anchors}, 'seed': seed, 'project': project, 'name': 'best-per-class'},\n",
        "        {'seed': seed},\n",
        "        extract_model_fn=lambda lit_model: lit_model.student\n",
        "    )\n",
        "    id = {'id': wandb.run.id} if project is not None else {}\n",
        "    # distilled_best_per_class_results.append(mnist_ssl_sk_test(model=lit_distilled_best_per_class.student, linear=distilled_best_per_class_linear, project=project, *id))\n",
        "    # can't get the partial function to work when using wandb, so call original `sk_test` function\n",
        "    distilled_best_per_class_results.append(sk_test(\n",
        "        model=lit_distilled_best_per_class.student,\n",
        "        linear=distilled_best_per_class_linear,\n",
        "        dm_init=GenericDataModule,\n",
        "        dm_kwargs=mnist_1024,\n",
        "        model_preprocess=lambda model: model[:5],\n",
        "        project=project,\n",
        "        id=id.get('id')\n",
        "    ))\n",
        "  return distilled_results, distilled_rand_results, distilled_per_class_results, distilled_best_per_class_results"
      ],
      "metadata": {
        "id": "hJWm3L8qx_wH",
        "scrolled": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "in_batch, rand, rand_per_class, best_per_class = anchor_experiment([42], project='rel-rep-dist-anc-sel')"
      ],
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "id": "G2pfauGVKsTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[np.mean(scores) for scores in (in_batch, rand, rand_per_class, best_per_class)]"
      ],
      "metadata": {
        "id": "UxWr3tZQfwtS",
        "scrolled": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparison to other methods"
      ],
      "metadata": {
        "id": "4ja-fXFQVzif"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SL CIFAR10"
      ],
      "metadata": {
        "id": "FLM2WVtPEWwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lit_teacher = cifar10_sl_quick_train(\n",
        "    create_cifar10_cls_teacher,\n",
        "    lit_model_kwargs={'lr': 1e-1, 'num_classes': 10},\n",
        "    trainer_kwargs={'max_epochs': 50},\n",
        "    project='rel-rep-dist-comp-cifar10-tune',\n",
        "    name='teacher',\n",
        "    log_model=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "shiJydzzEWwk",
        "scrolled": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quick_lit_test(\n",
        "    lit_teacher,\n",
        "    dm_init=GenericDataModule,\n",
        "    dm_kwargs={'ds_class': CIFAR10, 'batch_size': 128},\n",
        "    project='rel-rep-dist-comp-cifar10-tune',\n",
        ")"
      ],
      "metadata": {
        "id": "yR64wIS3Niy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lit_rr_distilled, rr_distilled_linear = quick_fn_probe(\n",
        "    cifar10_sl_quick_distill,\n",
        "    cifar10_sl_quick_sk_probe,\n",
        "    {\n",
        "        'distill_kwargs': {\n",
        "          'teacher': strip_resnet_cls_head(lit_teacher.model),\n",
        "          'lr': 1e-1,\n",
        "          'ce_weight': 1,\n",
        "          'dim': 256,\n",
        "          'num_classes': 10,\n",
        "          'non_linear_head': False,\n",
        "          'dropout_head': False\n",
        "        },\n",
        "        'trainer_kwargs': {'max_epochs': 50},\n",
        "        'project': 'rel-rep-dist-comp-cifar10-tune',\n",
        "        'name': 'rel-rep momentum=0.9',\n",
        "        'log_model': True\n",
        "    },\n",
        "    extract_model_fn=extract_student,\n",
        ")"
      ],
      "metadata": {
        "id": "5uuUrCVGEWwl",
        "scrolled": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quick_sk_test(\n",
        "    lit_rr_distilled.student,\n",
        "    rr_distilled_linear,\n",
        "    dm_init=GenericDataModule,\n",
        "    dm_kwargs={'ds_class': CIFAR10, 'batch_size': 128},\n",
        "    project='rel-rep-dist-comp-cifar10-tune',\n",
        ")"
      ],
      "metadata": {
        "id": "i7D2eCQvPbYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lit_sp_distilled = cifar10_sl_quick_distill(\n",
        "    distill_cls=LitSPDistiller,\n",
        "    distill_kwargs={\n",
        "        'teacher': strip_resnet_cls_head(lit_teacher.model),\n",
        "        'lr': 1e-1,\n",
        "        'momentum': 0.9,\n",
        "        'd_weight': 1,\n",
        "        'ce_weight': 1,\n",
        "        'num_classes': 10,\n",
        "        'dim': 256\n",
        "    },\n",
        "    trainer_kwargs={'max_epochs': 50},\n",
        "    project='rel-rep-dist-comp-cifar10-tune',\n",
        "    name='sim-pre log model',\n",
        "    log_model=True\n",
        ")"
      ],
      "metadata": {
        "id": "6r_mCpgKEWwl",
        "scrolled": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quick_lit_test(\n",
        "    LitModel(nn.Sequential(lit_sp_distilled.student, lit_sp_distilled.head), num_classes=10),\n",
        "    dm_init=GenericDataModule,\n",
        "    dm_kwargs={'ds_class': CIFAR10, 'batch_size': 128},\n",
        "    project='rel-rep-dist-comp-cifar10-tune',\n",
        ")"
      ],
      "metadata": {
        "id": "UCh4TSQAo0kv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lit_lp_distilled = cifar10_sl_quick_distill(\n",
        "    distill_cls=LitLPDistiller,\n",
        "    distill_kwargs={\n",
        "        'teacher': strip_resnet_cls_head(lit_teacher.model),\n",
        "        'lr': 1e-1,\n",
        "        'normalizing_constant': 1,\n",
        "        'dim': 256,\n",
        "        'num_classes': 10,\n",
        "        'teacher_head': lit_teacher.model.linear\n",
        "    },\n",
        "    trainer_kwargs={'max_epochs': 50},\n",
        "    project='rel-rep-dist-comp-cifar10-tune',\n",
        "    name='loc-pre reprod',\n",
        "    log_model=True\n",
        ")"
      ],
      "metadata": {
        "id": "SrKKT_jcEWwl",
        "scrolled": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quick_lit_test(\n",
        "    LitModel(nn.Sequential(lit_lp_distilled.student, lit_lp_distilled.student_head), num_classes=10),\n",
        "    dm_init=GenericDataModule,\n",
        "    dm_kwargs={'ds_class': CIFAR10, 'batch_size': 128},\n",
        "    project='rel-rep-dist-comp-cifar10-tune',\n",
        ")"
      ],
      "metadata": {
        "id": "AH3odRwtyIZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SL CIFAR100\n"
      ],
      "metadata": {
        "id": "jybUcA7Sa2df"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_cifar100_cls_teacher():\n",
        "  return ResNetGenerator('resnet-18', num_classes=100)\n",
        "\n",
        "def create_cifar100_cls_student():\n",
        "  return ResNetGenerator('resnet-9', num_classes=100, width=0.5)"
      ],
      "metadata": {
        "scrolled": true,
        "id": "3nWRQ4b1weT_",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cifar100_sl_quick_train = partial(\n",
        "    quick_train,\n",
        "    lit_model_cls=LitModel,\n",
        "    dm_init=GenericDataModule,\n",
        "    dm_kwargs={'ds_class': CIFAR100, 'batch_size': 128},\n",
        "    trainer_kwargs={'max_epochs': 10}\n",
        ")\n",
        "\n",
        "cifar100_sl_quick_distill = partial(\n",
        "    quick_distill,\n",
        "    student_init=create_cifar100_cls_student,\n",
        "    dm_init=GenericDataModule,\n",
        "    dm_kwargs={'ds_class': CIFAR100, 'batch_size': 128},\n",
        "    trainer_kwargs={'max_epochs': 10},\n",
        "    student_preprocess=strip_resnet_cls_head\n",
        ")\n",
        "\n",
        "cifar100_sl_quick_sk_probe = partial(\n",
        "    quick_sk_probe,\n",
        "    dm_init=GenericDataModule,\n",
        "    dm_kwargs={'ds_class': CIFAR100, 'batch_size': 1024},\n",
        ")\n",
        "\n",
        "cifar100_sl_quick_fc_probe = partial(\n",
        "    quick_fc_probe,\n",
        "    dm_init=GenericDataModule,\n",
        "    dm_kwargs={'ds_class': CIFAR100, 'batch_size': 1024},\n",
        ")\n",
        "\n",
        "cifar100_sl_sk_test = partial(\n",
        "    quick_sk_test,\n",
        "    dm_init=GenericDataModule,\n",
        "    dm_kwargs={'ds_class': CIFAR100, 'batch_size': 1024},\n",
        ")"
      ],
      "metadata": {
        "scrolled": true,
        "id": "Xum9Xgs9weT_",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lit_teacher = cifar100_sl_quick_train(\n",
        "  create_cifar100_cls_teacher,\n",
        "  lit_model_kwargs={'lr': 1e-1, 'num_classes': 100},\n",
        "  trainer_kwargs={'max_epochs': 50},\n",
        "  project='rel-rep-dist-comp-cifar100-tune',\n",
        "  name='teacher aug decay 0.2 every 10',\n",
        "  log_model=True\n",
        ")"
      ],
      "metadata": {
        "id": "BpkM4mLya2dg",
        "scrolled": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quick_lit_test(\n",
        "    lit_teacher,\n",
        "    GenericDataModule,\n",
        "    {'ds_class': CIFAR100, 'batch_size': 128},\n",
        "    project='rel-rep-dist-comp-cifar100-tune',\n",
        ")"
      ],
      "metadata": {
        "id": "m7ryFqmEtz4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lit_rr_distilled, rr_distilled_linear = quick_fn_probe(\n",
        "    cifar100_sl_quick_distill,\n",
        "    cifar100_sl_quick_sk_probe,\n",
        "    {\n",
        "        'distill_kwargs': {\n",
        "          'teacher': strip_resnet_cls_head(lit_teacher.model),\n",
        "          'lr': 1e-1,\n",
        "          'ce_weight': 1,\n",
        "          'dim': 256,\n",
        "          'num_classes': 100,\n",
        "          'non_linear_head': False,\n",
        "          'dropout_head': False,\n",
        "        },\n",
        "        'trainer_kwargs': {'max_epochs': 50},\n",
        "        'project': 'rel-rep-dist-comp-cifar100-tune',\n",
        "        'name': 'rel-rep momentum=0.9',\n",
        "        'log_model': True\n",
        "    },\n",
        "    extract_model_fn=extract_student,\n",
        ")"
      ],
      "metadata": {
        "id": "rZdTBMfqa2dg",
        "scrolled": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cifar100_sl_sk_test(\n",
        "    lit_rr_distilled.student,\n",
        "    rr_distilled_linear,\n",
        "    project='rel-rep-dist-comp-cifar100-tune',\n",
        ")"
      ],
      "metadata": {
        "id": "vj-DNSge1KEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lit_sp_distilled = cifar100_sl_quick_distill(\n",
        "    distill_cls=LitSPDistiller,\n",
        "    distill_kwargs={\n",
        "        'teacher': strip_resnet_cls_head(lit_teacher.model),\n",
        "        'lr': 1e-1,\n",
        "        'momentum': 0.9,\n",
        "        'd_weight': 1,\n",
        "        'ce_weight': 1,\n",
        "        'num_classes': 100,\n",
        "        'dim': 256\n",
        "    },\n",
        "    trainer_kwargs={'max_epochs': 50},\n",
        "    project='rel-rep-dist-comp-cifar100-tune',\n",
        "    name='sim-pre no labels',\n",
        "    log_model=True\n",
        ")"
      ],
      "metadata": {
        "id": "MkcVam3aa2dh",
        "scrolled": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quick_lit_test(\n",
        "    LitModel(nn.Sequential(lit_sp_distilled.student, lit_sp_distilled.head), num_classes=100),\n",
        "    dm_init=GenericDataModule,\n",
        "    dm_kwargs={'ds_class': CIFAR100, 'batch_size': 128},\n",
        "    project='rel-rep-dist-comp-cifar100-tune',\n",
        ")"
      ],
      "metadata": {
        "id": "Ir6zuVDAHH5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lit_lp_distilled = cifar100_sl_quick_distill(\n",
        "    distill_cls=LitLPDistiller,\n",
        "    distill_kwargs={\n",
        "        'teacher': strip_resnet_cls_head(lit_teacher.model),\n",
        "        'lr': 1e-1,\n",
        "        'normalizing_constant': 1,\n",
        "        'dim': 256,\n",
        "        'num_classes': 100,\n",
        "        'teacher_head': lit_teacher.model.linear\n",
        "    },\n",
        "    trainer_kwargs={'max_epochs': 50},\n",
        "    project='rel-rep-dist-comp-cifar100-tune',\n",
        "    name='loc-pre',\n",
        "    log_model=True\n",
        ")"
      ],
      "metadata": {
        "id": "9D6HFBaha2dh",
        "scrolled": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quick_lit_test(\n",
        "    LitModel(nn.Sequential(lit_lp_distilled.student, lit_lp_distilled.student_head), num_classes=100),\n",
        "    dm_init=GenericDataModule,\n",
        "    dm_kwargs={'ds_class': CIFAR100, 'batch_size': 128},\n",
        "    project='rel-rep-dist-comp-cifar100-tune',\n",
        ")"
      ],
      "metadata": {
        "id": "pbRtxaKFTUDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SL SVHN"
      ],
      "metadata": {
        "id": "QQ7GEkS-weT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_svhn_cls_teacher():\n",
        "  return ResNetGenerator('resnet-18', num_classes=10)\n",
        "\n",
        "def create_svhn_cls_student():\n",
        "  return ResNetGenerator('resnet-9', num_classes=10, width=0.5)\n",
        "\n",
        "svhn_sl_quick_train = partial(\n",
        "    quick_train,\n",
        "    lit_model_cls=LitModel,\n",
        "    dm_init=GenericDataModule,\n",
        "    dm_kwargs={'ds_class': SVHN, 'batch_size': 128},\n",
        "    trainer_kwargs={'max_epochs': 50}\n",
        ")\n",
        "\n",
        "svhn_sl_quick_distill = partial(\n",
        "    quick_distill,\n",
        "    student_init=create_svhn_cls_student,\n",
        "    dm_init=GenericDataModule,\n",
        "    dm_kwargs={'ds_class': SVHN, 'batch_size': 128},\n",
        "    trainer_kwargs={'max_epochs': 50},\n",
        "    student_preprocess=strip_resnet_cls_head\n",
        ")\n",
        "\n",
        "svhn_sl_quick_sk_probe = partial(\n",
        "    quick_sk_probe,\n",
        "    dm_init=GenericDataModule,\n",
        "    dm_kwargs={'ds_class': SVHN, 'batch_size': 1024},\n",
        ")\n",
        "\n",
        "svhn_sl_quick_fc_probe = partial(\n",
        "    quick_fc_probe,\n",
        "    dm_init=GenericDataModule,\n",
        "    dm_kwargs={'ds_class': SVHN, 'batch_size': 1024},\n",
        ")\n",
        "\n",
        "svhn_sl_sk_test = partial(\n",
        "    quick_sk_test,\n",
        "    dm_init=GenericDataModule,\n",
        "    dm_kwargs={'ds_class': SVHN, 'batch_size': 1024},\n",
        ")\n",
        "\n",
        "svhn_sl_quick_lit_test = partial(\n",
        "    quick_lit_test,\n",
        "    dm_init=GenericDataModule,\n",
        "    dm_kwargs={'ds_class': SVHN, 'batch_size': 1024},\n",
        ")"
      ],
      "metadata": {
        "id": "PIDTPAjvweT_",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lit_teacher = svhn_sl_quick_train(\n",
        "  create_svhn_cls_teacher,\n",
        "  lit_model_kwargs={'lr': 1e-1, 'num_classes': 10},\n",
        "  trainer_kwargs={'max_epochs': 20},\n",
        "  project='rel-rep-dist-comp-svhn-tune',\n",
        "  name='teacher',\n",
        "  log_model=True\n",
        ")"
      ],
      "metadata": {
        "id": "as6fRYR8weUA",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svhn_sl_quick_lit_test(\n",
        "    lit_teacher,\n",
        "    project='rel-rep-dist-comp-svhn-tune'\n",
        ")"
      ],
      "metadata": {
        "id": "3j8YCfTiPpAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lit_rr_distilled, rr_distilled_linear = quick_fn_probe(\n",
        "    svhn_sl_quick_distill,\n",
        "    svhn_sl_quick_sk_probe,\n",
        "    {\n",
        "        'distill_kwargs': {\n",
        "          'teacher': strip_resnet_cls_head(lit_teacher.model),\n",
        "          'lr': 1e-1,\n",
        "          'ce_weight': 1,\n",
        "          'dim': 256,\n",
        "          'num_classes': 10,\n",
        "          'non_linear_head': False,\n",
        "          'dropout_head': False,\n",
        "        },\n",
        "        'trainer_kwargs': {'max_epochs': 20},\n",
        "        'project': 'rel-rep-dist-comp-svhn-tune',\n",
        "        'name': 'rel-rep momentum=0.9',\n",
        "        'log_model': True\n",
        "    },\n",
        "    extract_model_fn=extract_student,\n",
        ")"
      ],
      "metadata": {
        "id": "JYgcSkyoweUA",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svhn_sl_sk_test(\n",
        "    lit_rr_distilled.student,\n",
        "    rr_distilled_linear,\n",
        "    project='rel-rep-dist-comp-svhn-tune',\n",
        ")"
      ],
      "metadata": {
        "id": "x1jU6Xn4LwBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lit_sp_distilled = svhn_sl_quick_distill(\n",
        "    distill_cls=LitSPDistiller,\n",
        "    distill_kwargs={\n",
        "        'teacher': strip_resnet_cls_head(lit_teacher.model),\n",
        "        'lr': 1e-1,\n",
        "        'momentum': 0.9,\n",
        "        'd_weight': 1,\n",
        "        'ce_weight': 1,\n",
        "        'num_classes': 10,\n",
        "        'dim': 256\n",
        "    },\n",
        "    trainer_kwargs={'max_epochs': 20},\n",
        "    project='rel-rep-dist-comp-svhn-tune',\n",
        "    name='sim-pre',\n",
        "    log_model=True\n",
        ")"
      ],
      "metadata": {
        "id": "La-axKt1weUA",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svhn_sl_quick_lit_test(\n",
        "    LitModel(nn.Sequential(lit_sp_distilled.student, lit_sp_distilled.head), num_classes=10),\n",
        "    project='rel-rep-dist-comp-svhn-tune',\n",
        ")"
      ],
      "metadata": {
        "id": "JzXMYek_RmGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lit_lp_distilled = svhn_sl_quick_distill(\n",
        "    distill_cls=LitLPDistiller,\n",
        "    distill_kwargs={\n",
        "        'teacher': strip_resnet_cls_head(lit_teacher.model),\n",
        "        'lr': 1e-1,\n",
        "        'normalizing_constant': 1,\n",
        "        'dim': 256,\n",
        "        'num_classes': 10,\n",
        "        'teacher_head': lit_teacher.model.linear\n",
        "    },\n",
        "    trainer_kwargs={'max_epochs': 20},\n",
        "    project='rel-rep-dist-comp-svhn-tune',\n",
        "    name='loc-pre fixed loss fn',\n",
        "    log_model=True\n",
        ")"
      ],
      "metadata": {
        "id": "K-6fdZhIweUA",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svhn_sl_quick_lit_test(\n",
        "    LitModel(nn.Sequential(lit_lp_distilled.student, lit_lp_distilled.student_head), num_classes=10),\n",
        "    project='rel-rep-dist-comp-svhn-tune',\n",
        ")"
      ],
      "metadata": {
        "id": "RzS7t5T8TBTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Stanford Cars"
      ],
      "metadata": {
        "id": "VM9zkxlnDGx_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p stanford_cars\n",
        "\n",
        "!wget https://web.archive.org/web/20230405013536/https://ai.stanford.edu/~jkrause/cars/car_devkit.tgz -P stanford_cars\n",
        "!tar -xzf stanford_cars/car_devkit.tgz -C stanford_cars\n",
        "\n",
        "!wget https://web.archive.org/web/20230405013536/http://ai.stanford.edu/~jkrause/car196/cars_train.tgz -P stanford_cars\n",
        "!tar -xzf stanford_cars/cars_train.tgz -C stanford_cars\n",
        "\n",
        "!wget https://web.archive.org/web/20230405013536/http://ai.stanford.edu/~jkrause/car196/cars_test.tgz -P stanford_cars\n",
        "!tar -xzf stanford_cars/cars_test.tgz -C stanford_cars\n",
        "\n",
        "!wget https://web.archive.org/web/20230405013536/http://ai.stanford.edu/~jkrause/car196/cars_test_annos_withlabels.mat -P stanford_cars"
      ],
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "id": "Tog8Wf_WDGyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def strip_timm_resnet_cls_head(model):\n",
        "    return nn.Sequential(*list(model.children())[:-1])\n",
        "\n",
        "create_cars_cls_teacher = lambda : timm.create_model('resnet34', pretrained=True, num_classes=196)\n",
        "create_cars_cls_student = lambda : timm.create_model('resnet18', pretrained=True, num_classes=196)"
      ],
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "id": "J6Kejhk8DGyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cars_sl_quick_train = partial(\n",
        "    quick_train,\n",
        "    lit_model_cls=LitModel,\n",
        "    dm_init=GenericDataModule,\n",
        "    dm_kwargs={'ds_class': StanfordCars, 'timm_name': 'resnet34', 'batch_size': 128},\n",
        "    trainer_kwargs={'max_epochs': 20}\n",
        ")\n",
        "\n",
        "cars_sl_quick_distill = partial(\n",
        "    quick_distill,\n",
        "    student_init=create_cars_cls_student,\n",
        "    dm_init=GenericDataModule,\n",
        "    dm_kwargs={'ds_class': StanfordCars, 'timm_name': 'resnet18', 'batch_size': 128}, # pretty sure transforms are the same for both resnet34 and reset18\n",
        "    trainer_kwargs={'max_epochs': 20},\n",
        "    student_preprocess=strip_timm_resnet_cls_head\n",
        ")\n",
        "\n",
        "cars_sl_quick_fc_probe = partial(\n",
        "    quick_fc_probe,\n",
        "    dm_init=GenericDataModule,\n",
        "    dm_kwargs={'ds_class': StanfordCars, 'timm_name': 'resnet18', 'batch_size': 128},\n",
        ")\n",
        "\n",
        "cars_sl_quick_sk_probe = partial(\n",
        "    quick_sk_probe,\n",
        "    dm_init=GenericDataModule,\n",
        "    dm_kwargs={'ds_class': StanfordCars, 'timm_name': 'resnet18', 'batch_size': 128},\n",
        ")"
      ],
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "id": "PUL73LvUDGyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lit_teacher = cars_sl_quick_train(\n",
        "    create_cars_cls_teacher,\n",
        "    lit_model_kwargs={'lr': 1e-1, 'num_classes': 196},\n",
        "    project='rel-rep-dist-comp-cars-tune',\n",
        "    name='teacher resnet-34',\n",
        "    log_model=True\n",
        ")"
      ],
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "id": "nn7giEgdDGyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quick_lit_test(\n",
        "    lit_teacher,\n",
        "    GenericDataModule,\n",
        "    {'ds_class': StanfordCars, 'timm_name': 'resnet34', 'batch_size': 128},\n",
        "    project='rel-rep-dist-comp-cars-tune',\n",
        ")"
      ],
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "id": "KfcurmDTC_Zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lit_rr_distilled, rr_distilled_linear = quick_fn_probe(\n",
        "    cars_sl_quick_distill,\n",
        "    cars_sl_quick_sk_probe,\n",
        "    {\n",
        "        'distill_kwargs': {\n",
        "          'teacher': strip_timm_resnet_cls_head(lit_teacher.model),\n",
        "          'lr': 1e-1,\n",
        "          'dim': 512,\n",
        "          'num_classes': 196,\n",
        "          'ce_weight': 1,\n",
        "          'non_linear_head': False,\n",
        "          'dropout_head': False\n",
        "        },\n",
        "        'project': 'rel-rep-dist-comp-cars-tune',\n",
        "        'name': 'rel-rep momentum=0.9',\n",
        "        'log_model': True\n",
        "    },\n",
        "    extract_model_fn=extract_student,\n",
        ")"
      ],
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "id": "hBecEwxPDGyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quick_sk_test(\n",
        "    lit_rr_distilled.student,\n",
        "    rr_distilled_linear,\n",
        "    dm_init=GenericDataModule,\n",
        "    dm_kwargs={'ds_class': StanfordCars, 'timm_name': 'resnet34', 'batch_size': 128},\n",
        "    project='rel-rep-dist-comp-cars-tune',\n",
        ")"
      ],
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "id": "_6LUnd7_C_Zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lit_sp_distilled = cars_sl_quick_distill(\n",
        "    distill_cls=LitSPDistiller,\n",
        "    distill_kwargs={\n",
        "        'teacher': strip_timm_resnet_cls_head(lit_teacher.model),\n",
        "        'lr': 1e-1,\n",
        "        'momentum': 0.9,\n",
        "        'd_weight': 1,\n",
        "        'ce_weight': 1,\n",
        "        'num_classes': 196,\n",
        "        'dim': 512\n",
        "    },\n",
        "    project='rel-rep-dist-comp-cars-tune',\n",
        "    name='sim-pre correct teacher head strip',\n",
        "    log_model=True\n",
        ")"
      ],
      "metadata": {
        "scrolled": true,
        "id": "hFQVs_1vweUA",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quick_lit_test(\n",
        "    LitModel(nn.Sequential(lit_sp_distilled.student, lit_sp_distilled.head), num_classes=196),\n",
        "    GenericDataModule,\n",
        "    {'ds_class': StanfordCars, 'timm_name': 'resnet34', 'batch_size': 128},\n",
        "    project='rel-rep-dist-comp-cars-tune',\n",
        ")"
      ],
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "id": "_gg6qOhCC_Zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lit_lp_distilled = cars_sl_quick_distill(\n",
        "    distill_cls=LitLPDistiller,\n",
        "    distill_kwargs={\n",
        "        'teacher': strip_timm_resnet_cls_head(lit_teacher.model),\n",
        "        'lr': 1e-1,\n",
        "        'normalizing_constant': 1,\n",
        "        'dim': 512,\n",
        "        'num_classes': 196,\n",
        "        'teacher_head': lit_teacher.model.fc\n",
        "    },\n",
        "    project='rel-rep-dist-comp-cars-tune',\n",
        "    name='loc-pre',\n",
        "    log_model=True\n",
        ")"
      ],
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "id": "PDpOr0scC_Zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quick_lit_test(\n",
        "    LitModel(nn.Sequential(lit_lp_distilled.student, lit_lp_distilled.student_head), num_classes=196),\n",
        "    GenericDataModule,\n",
        "    {'ds_class': StanfordCars, 'timm_name': 'resnet34', 'batch_size': 128},\n",
        "    project='rel-rep-dist-comp-cars-tune',\n",
        ")"
      ],
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "id": "LRO9G5v5C_Zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Oxford Pet"
      ],
      "metadata": {
        "id": "LA1O9z5oC_Zt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def strip_timm_resnet_cls_head(model):\n",
        "    return nn.Sequential(*list(model.children())[:-1])\n",
        "\n",
        "create_pet_cls_teacher = lambda : timm.create_model('resnet34', pretrained=True, num_classes=37)\n",
        "create_pet_cls_student = lambda : timm.create_model('resnet18', pretrained=True, num_classes=37)\n",
        "\n",
        "pet_sl_quick_train = partial(\n",
        "    quick_train,\n",
        "    lit_model_cls=LitModel,\n",
        "    dm_init=GenericDataModule,\n",
        "    dm_kwargs={'ds_class': OxfordIIITPet, 'timm_name': 'resnet34', 'batch_size': 128},\n",
        "    trainer_kwargs={'max_epochs': 20}\n",
        ")\n",
        "\n",
        "pet_sl_quick_distill = partial(\n",
        "    quick_distill,\n",
        "    student_init=create_pet_cls_student,\n",
        "    dm_init=GenericDataModule,\n",
        "    dm_kwargs={'ds_class': OxfordIIITPet, 'timm_name': 'resnet18', 'batch_size': 128}, # pretty sure transforms are the same for both resnet34 and reset18\n",
        "    trainer_kwargs={'max_epochs': 20},\n",
        "    student_preprocess=strip_timm_resnet_cls_head\n",
        ")\n",
        "\n",
        "pet_sl_quick_fc_probe = partial(\n",
        "    quick_fc_probe,\n",
        "    dm_init=GenericDataModule,\n",
        "    dm_kwargs={'ds_class': OxfordIIITPet, 'timm_name': 'resnet18', 'batch_size': 128},\n",
        ")\n",
        "\n",
        "pet_sl_quick_sk_probe = partial(\n",
        "    quick_sk_probe,\n",
        "    dm_init=GenericDataModule,\n",
        "    dm_kwargs={'ds_class': OxfordIIITPet, 'timm_name': 'resnet18', 'batch_size': 128},\n",
        ")\n",
        "\n",
        "pet_sl_quick_sk_test = partial(\n",
        "    quick_sk_test,\n",
        "    dm_init=GenericDataModule,\n",
        "    dm_kwargs={'ds_class': OxfordIIITPet, 'timm_name': 'resnet18', 'batch_size': 128}\n",
        ")"
      ],
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "id": "L-lwNQhcC_Zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lit_teacher = pet_sl_quick_train(\n",
        "    create_pet_cls_teacher,\n",
        "    lit_model_kwargs={'lr': 1e-1, 'num_classes': 37},\n",
        "    project='rel-rep-dist-comp-pet-tune',\n",
        "    name='teacher resnet-34',\n",
        "    log_model=True\n",
        ")"
      ],
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "id": "eXt7JoXAC_Zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quick_lit_test(\n",
        "    lit_teacher,\n",
        "    GenericDataModule,\n",
        "    {'ds_class': OxfordIIITPet, 'timm_name': 'resnet18', 'batch_size': 128},\n",
        "    project='rel-rep-dist-comp-pet-tune',\n",
        ")"
      ],
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "id": "VhXey1VsC_Zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lit_rr_distilled, rr_distilled_linear = quick_fn_probe(\n",
        "    pet_sl_quick_distill,\n",
        "    pet_sl_quick_sk_probe,\n",
        "    {\n",
        "        'distill_kwargs': {\n",
        "          'teacher': strip_timm_resnet_cls_head(lit_teacher.model),\n",
        "          'lr': 1e-1,\n",
        "          'dim': 512,\n",
        "          'num_classes': 37,\n",
        "          'ce_weight': 1,\n",
        "          'non_linear_head': False,\n",
        "          'dropout_head': False\n",
        "        },\n",
        "        'project': 'rel-rep-dist-comp-pet-tune',\n",
        "        'name': 'rel-rep momentum=0.9',\n",
        "        'log_model': True\n",
        "    },\n",
        "    extract_model_fn=extract_student,\n",
        ")"
      ],
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "id": "EV905yQWC_Zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pet_sl_quick_sk_test(\n",
        "    lit_rr_distilled.student,\n",
        "    rr_distilled_linear,\n",
        "    project='rel-rep-dist-comp-pet-tune',\n",
        ")"
      ],
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "id": "OtgqlCv_C_Zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lit_sp_distilled = pet_sl_quick_distill(\n",
        "    distill_cls=LitSPDistiller,\n",
        "    distill_kwargs={\n",
        "        'teacher': strip_timm_resnet_cls_head(lit_teacher.model),\n",
        "        'lr': 1e-1,\n",
        "        'momentum': 0.9,\n",
        "        'd_weight': 1,\n",
        "        'ce_weight': 1,\n",
        "        'num_classes': 37,\n",
        "        'dim': 512\n",
        "    },\n",
        "    project='rel-rep-dist-comp-pet-tune',\n",
        "    name='sim-pre',\n",
        "    log_model=True\n",
        ")"
      ],
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "id": "YCfcdzXDC_Zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quick_lit_test(\n",
        "    LitModel(nn.Sequential(lit_sp_distilled.student, lit_sp_distilled.head), num_classes=37),\n",
        "    GenericDataModule,\n",
        "    {'ds_class': OxfordIIITPet, 'timm_name': 'resnet18', 'batch_size': 128},\n",
        "    project='rel-rep-dist-comp-pet-tune',\n",
        ")"
      ],
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "id": "qUZ9l5BoC_Zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lit_lp_distilled = pet_sl_quick_distill(\n",
        "    distill_cls=LitLPDistiller,\n",
        "    distill_kwargs={\n",
        "        'teacher': strip_timm_resnet_cls_head(lit_teacher.model),\n",
        "        'lr': 1e-1,\n",
        "        'normalizing_constant': 1,\n",
        "        'dim': 512,\n",
        "        'num_classes': 37,\n",
        "        'teacher_head': lit_teacher.model.fc\n",
        "    },\n",
        "    project='rel-rep-dist-comp-pet-tune',\n",
        "    name='loc-pre',\n",
        "    log_model=True\n",
        ")"
      ],
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "id": "NRrQjRDSC_Zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quick_lit_test(\n",
        "    LitModel(nn.Sequential(lit_lp_distilled.student, lit_lp_distilled.student_head), num_classes=37),\n",
        "    GenericDataModule,\n",
        "    {'ds_class': OxfordIIITPet, 'timm_name': 'resnet18', 'batch_size': 128},\n",
        "    project='rel-rep-dist-comp-pet-tune',\n",
        ")"
      ],
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "id": "DbgdoSPWC_Zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Oxford Flowers"
      ],
      "metadata": {
        "id": "_XAdXD0HC_Zu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def strip_timm_resnet_cls_head(model):\n",
        "    return nn.Sequential(*list(model.children())[:-1])\n",
        "\n",
        "create_flowers_cls_teacher = lambda : timm.create_model('resnet34', pretrained=True, num_classes=102)\n",
        "create_flowers_cls_student = lambda : timm.create_model('resnet18', pretrained=True, num_classes=102)\n",
        "\n",
        "flowers_sl_quick_train = partial(\n",
        "    quick_train,\n",
        "    lit_model_cls=LitModel,\n",
        "    dm_init=GenericDataModule,\n",
        "    dm_kwargs={'ds_class': Flowers102, 'timm_name': 'resnet34', 'batch_size': 128},\n",
        "    trainer_kwargs={'max_epochs': 50}\n",
        ")\n",
        "\n",
        "flowers_sl_quick_distill = partial(\n",
        "    quick_distill,\n",
        "    student_init=create_flowers_cls_student,\n",
        "    dm_init=GenericDataModule,\n",
        "    dm_kwargs={'ds_class': Flowers102, 'timm_name': 'resnet18', 'batch_size': 128}, # pretty sure transforms are the same for both resnet34 and reset18\n",
        "    trainer_kwargs={'max_epochs': 50},\n",
        "    student_preprocess=strip_timm_resnet_cls_head\n",
        ")\n",
        "\n",
        "flowers_sl_quick_fc_probe = partial(\n",
        "    quick_fc_probe,\n",
        "    dm_init=GenericDataModule,\n",
        "    dm_kwargs={'ds_class': Flowers102, 'timm_name': 'resnet18', 'batch_size': 128},\n",
        ")\n",
        "\n",
        "flowers_sl_quick_sk_probe = partial(\n",
        "    quick_sk_probe,\n",
        "    dm_init=GenericDataModule,\n",
        "    dm_kwargs={'ds_class': Flowers102, 'timm_name': 'resnet18', 'batch_size': 128},\n",
        ")\n",
        "\n",
        "flowers_sl_quick_sk_test = partial(\n",
        "    quick_sk_test,\n",
        "    dm_init=GenericDataModule,\n",
        "    dm_kwargs={'ds_class': Flowers102, 'timm_name': 'resnet18', 'batch_size': 128}\n",
        ")\n",
        "\n",
        "flowers_sl_quick_lit_test = partial(\n",
        "    quick_lit_test,\n",
        "    dm_init=GenericDataModule,\n",
        "    dm_kwargs={'ds_class': Flowers102, 'timm_name': 'resnet18', 'batch_size': 128}\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-02T07:56:06.208603Z",
          "iopub.execute_input": "2023-05-02T07:56:06.209409Z",
          "iopub.status.idle": "2023-05-02T07:56:06.220911Z",
          "shell.execute_reply.started": "2023-05-02T07:56:06.209367Z",
          "shell.execute_reply": "2023-05-02T07:56:06.219645Z"
        },
        "scrolled": true,
        "trusted": true,
        "id": "5hiSm4bkC_Zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lit_teacher = flowers_sl_quick_train(\n",
        "    create_flowers_cls_teacher,\n",
        "    lit_model_kwargs={'lr': 1e-1, 'num_classes': 102},\n",
        "    project='rel-rep-dist-comp-flowers-tune',\n",
        "    name='teacher',\n",
        "    log_model=True,\n",
        "    trainer_kwargs={'max_epochs': 50}\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-02T07:56:07.555029Z",
          "iopub.execute_input": "2023-05-02T07:56:07.555844Z",
          "iopub.status.idle": "2023-05-02T07:56:08.549594Z",
          "shell.execute_reply.started": "2023-05-02T07:56:07.555791Z",
          "shell.execute_reply": "2023-05-02T07:56:08.548237Z"
        },
        "scrolled": true,
        "trusted": true,
        "id": "HsIAZcLkC_Zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flowers_sl_quick_lit_test(\n",
        "    lit_teacher,\n",
        "    project='rel-rep-dist-comp-flowers-tune',\n",
        ")"
      ],
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "id": "9r-x3XYqC_Zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lit_rr_distilled, rr_distilled_linear = quick_fn_probe(\n",
        "    flowers_sl_quick_distill,\n",
        "    flowers_sl_quick_sk_probe,\n",
        "    {\n",
        "        'distill_kwargs': {\n",
        "          'teacher': strip_timm_resnet_cls_head(lit_teacher.model),\n",
        "          'lr': 1e-1,\n",
        "          'dim': 512,\n",
        "          'num_classes': 102,\n",
        "          'ce_weight': 1,\n",
        "          'non_linear_head': False,\n",
        "          'dropout_head': False\n",
        "        },\n",
        "        'project': 'rel-rep-dist-comp-flowers-tune',\n",
        "        'name': 'rel-rep momentum=0.9',\n",
        "        'log_model': True\n",
        "    },\n",
        "    extract_model_fn=extract_student,\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-02T07:56:13.225799Z",
          "iopub.execute_input": "2023-05-02T07:56:13.226218Z",
          "iopub.status.idle": "2023-05-02T08:09:06.795163Z",
          "shell.execute_reply.started": "2023-05-02T07:56:13.226182Z",
          "shell.execute_reply": "2023-05-02T08:09:06.793844Z"
        },
        "scrolled": true,
        "trusted": true,
        "id": "_XL_HmCwC_Zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flowers_sl_quick_sk_test(\n",
        "    lit_rr_distilled.student,\n",
        "    rr_distilled_linear,\n",
        "    project='rel-rep-dist-comp-flowers-tune',\n",
        ")"
      ],
      "metadata": {
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2023-05-02T08:15:53.120417Z",
          "iopub.execute_input": "2023-05-02T08:15:53.120969Z",
          "iopub.status.idle": "2023-05-02T08:17:15.678863Z",
          "shell.execute_reply.started": "2023-05-02T08:15:53.120923Z",
          "shell.execute_reply": "2023-05-02T08:17:15.669152Z"
        },
        "trusted": true,
        "id": "R1dsLZ-XC_Zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lit_sp_distilled = flowers_sl_quick_distill(\n",
        "    distill_cls=LitSPDistiller,\n",
        "    distill_kwargs={\n",
        "        'teacher': strip_timm_resnet_cls_head(lit_teacher.model),\n",
        "        'lr': 1e-1,\n",
        "        'momentum': 0.9,\n",
        "        'd_weight': 1,\n",
        "        'ce_weight': 1,\n",
        "        'num_classes': 102,\n",
        "        'dim': 512\n",
        "    },\n",
        "    project='rel-rep-dist-comp-flowers-tune',\n",
        "    name='sim-pre',\n",
        "    log_model=True\n",
        ")"
      ],
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "id": "7v4QGlUPC_Zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flowers_sl_quick_lit_test(\n",
        "    LitModel(nn.Sequential(lit_sp_distilled.student, lit_sp_distilled.head), num_classes=102),\n",
        "    project='rel-rep-dist-comp-flowers-tune',\n",
        ")"
      ],
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "id": "swOXMuo3C_Zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lit_lp_distilled = flowers_sl_quick_distill(\n",
        "    distill_cls=LitLPDistiller,\n",
        "    distill_kwargs={\n",
        "        'teacher': strip_timm_resnet_cls_head(lit_teacher.model),\n",
        "        'lr': 1e-1,\n",
        "        'normalizing_constant': 1,\n",
        "        'dim': 512,\n",
        "        'num_classes': 102,\n",
        "        'teacher_head': lit_teacher.model.fc\n",
        "    },\n",
        "    project='rel-rep-dist-comp-flowers-tune',\n",
        "    name='loc-pre',\n",
        "    log_model=True\n",
        ")"
      ],
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "id": "u3iFRHi_C_Zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flowers_sl_quick_lit_test(\n",
        "    LitModel(nn.Sequential(lit_lp_distilled.student, lit_lp_distilled.student_head), num_classes=102),\n",
        "    project='rel-rep-dist-comp-flowers-tune',\n",
        ")"
      ],
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "id": "PbB6pZjJC_Zu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}